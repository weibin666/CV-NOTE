# 机器翻译论文学习

## 1、Transformer

- ### batchnorm和LayerNorm的区别

**BatchNorm**：batch方向做归一化，算NHW的均值，对小batchsize效果不好；BN主要缺点是对batchsize的大小比较敏感，由于每次计算均值和方差是在一个batch上，所以如果batchsize太小，则计算的均值、方差不足以代表整个数据分布
**LayerNorm**：channel方向做归一化，算CHW的均值，主要对RNN作用明显；



![img](https://img-blog.csdnimg.cn/20181230223445264.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3NoYW5nbGlhbmxt,size_16,color_FFFFFF,t_70)

#### 1)、 BatchNorm

```python
torch.nn.BatchNorm1d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
torch.nn.BatchNorm3d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
```

**参数：**

```python
num_features： 来自期望输入的特征数，该期望输入的大小为’batch_size x num_features [x width]’
eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。
momentum： 动态均值和动态方差所使用的动量。默认为0.1。
affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。
track_running_stats：布尔值，当设为true，记录训练过程中的均值和方差；
```

**实现公式：**

![在这里插入图片描述](https://img-blog.csdnimg.cn/20181225231336294.png)

#### 2)、 LayerNorm

`torch.nn.LayerNorm`(*normalized_shape*, *eps=1e-05*, *elementwise_affine=True*)

**参数：**

```python
normalized_shape： 输入尺寸
[∗×normalized_shape[0]×normalized_shape[1]×…×normalized_shape[−1]]
eps： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。
elementwise_affine： 布尔值，当设为true，给该层添加可学习的仿射变换参数。
```

**实现公式：**

![在这里插入图片描述](https://img-blog.csdnimg.cn/20181225232246856.png)

——————————————————————————————————————————————————
原文链接：https://blog.csdn.net/shanglianlm/article/details/85075706

参考链接：https://blog.csdn.net/liuxiao214/article/details/81037416

 听李沐视频的理解：

假设输入是一个二维样本：

![](images\BN-LN.jpg)

![BN-LN](images\BN.jpg)

![LN](images\LN.jpg)

为什么说LN在时序序列里面用的比较多，因为是时序序列里面，样本的长度可能会发生变化

![](images\BN-LN切法.jpg)

我们在预测的时候要把全局的方差和均值记录下来，假如预测的时候我们遇见一个特别长的样本，那使用BN的情况下，我们就么见过这么长的样本，所以预测会出现偏差。

decoder结构：

掩码mask只在训练的时候是有的，预测的时候不需要这个，因为在训练的时候我们t时刻的输入是以t时刻之前-的隐藏层h向量和当前时刻的输入token向量来确定当前时刻的词，所以不能看见未来的词，所以后面的词要进行mask掉。

2个向量的内积越大，就说明这俩向量的相似度就越高，内积为0说明这俩向量是正交的没有相似度了。

 1个query与

n个key和value pair，进行做内积，再进行softmax(每一行进行softmax)得到我们的一组权重值,，然后和value向量做乘积。

![](images\QKV.jpg)

![](images\KQV01.jpg)

为什么要除以根号dk?

答：因为当你的向量比较长的时候点积可能结果会比较大，这样做softmax的时候，结果会更加靠近1，剩下的那些值会靠近0，所以你的值会靠近两端，这样算梯度的时候你的梯度就会比较小，就会跑不动，所以一般在transformer中我们用dk比较大。



输入输出维度是一样的。



![](images\KQV02.jpg)

![](images\KQV03.jpg)

- 编码器最后的输出是n个长为d的向量，解码器mask模块后面的输出也是m个长为d的向量。

- 编码器最后的输出作为key和value进来，解码器multi-head attention&mask之后的输出作为query进来。

 ![](images\MLP.jpg)

- 所以这个地方MLP的作用就是做了一次汇聚aggration，因为之前的序列信息已经都有了。最后通过MLP将其加工成我们想要的那个语义空间语义向量。整个transformer就是这样提取时序信息的过程。