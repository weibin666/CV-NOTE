### 1、机器学习部分

前言：KNN、决策树、贝叶斯、逻辑回归、SVM、线性回归、树回归

1）监督学习：回归(线性回归、岭回归、多项式回归、决策树回归)、分类(二元逻辑分类、朴素贝叶斯分类、决策树分类、SVM)

2）非监督学习：聚类(掌握K-Means聚类)

#### ⑴、回归

##### ①、线性回归

线性回归相关API:

```python
import sklearn.linear_model as lm
# 创建模型
model = lm.LinearRegression()
# 训练模型
# 输入为一个二维数组表示的样本矩阵
# 输出为每个样本最终的结果
model.fit(输入, 输出) # 通过梯度下降法计算模型参数
# 预测输出  
# 输入array是一个二维数组，每一行是一个样本，每一列是一个特征。
result = model.predict(array)
```

评估训练结果误差（metrics）

```python
import sklearn.metrics as sm

# 平均绝对值误差：1/m∑|实际输出-预测输出|
sm.mean_absolute_error(y, pred_y)
# 平均平方误差：SQRT(1/mΣ(实际输出-预测输出)^2)
sm.mean_squared_error(y, pred_y)
# 中位绝对值误差：MEDIAN(|实际输出-预测输出|)
sm.median_absolute_error(y, pred_y)
# R2得分，(0,1]区间的分值。分数越高，误差越小。
sm.r2_score(y, pred_y)
```

模型的保存和加载：

```python
# 将训练好的模型对象保存到磁盘文件中
with open('../../data/linear.pkl', 'wb') as f:
    pickle.dump(model, f)
    
# 从磁盘文件中加载模型对象
with open('../../data/linear.pkl', 'rb') as f:
    model = pickle.load(f)
# 根据输入预测输出
pred_y = model.predict(x)
```

##### ②、岭回归：

> 普通线性回归模型使用基于梯度下降的最小二乘法，在最小化损失函数的前提下，寻找最优模型参数，于此过程中，包括少数异常样本在内的全部训练数据都会对最终模型参数造成程度相等的影响，异常值对模型所带来影响无法在训练过程中被识别出来。为此，岭回归在模型迭代过程所依据的**损失函数中增加了正则项**，以限制模型参数对异常样本的匹配程度，进而提高模型面对多数正常样本的拟合精度。

```python
import sklearn.linear_model as lm
# 创建模型
model = lm.Ridge(正则强度，fit_intercept=是否训练截距, max_iter=最大迭代次数)
# 训练模型
# 输入为一个二维数组表示的样本矩阵
# 输出为每个样本最终的结果
model.fit(输入, 输出)
# 预测输出  
# 输入array是一个二维数组，每一行是一个样本，每一列是一个特征。
result = model.predict(array)
```

##### ③、多项式回归：

```python
import sklearn.pipeline as pl
import sklearn.preprocessing as sp
import sklearn.linear_model as lm

model = pl.make_pipeline(
    sp.PolynomialFeatures(10),  # 多项式特征扩展器
    lm.LinearRegression())      # 线性回归器
```

##### ④、决策树回归：

```python
import sklearn.tree as st

# 创建决策树回归器模型  决策树的最大深度为4
model = st.DecisionTreeRegressor(max_depth=4)
# 训练模型  
# train_x： 二维数组样本数据
# train_y： 训练集中对应每行样本的结果
model.fit(train_x, train_y)
# 测试模型
pred_test_y = model.predict(test_x)
```

正向激励--构建出不同权重的若干棵决策树

```python
import sklearn.tree as st
import sklearn.ensemble as se
# model: 决策树模型（一颗）
model = st.DecisionTreeRegressor(max_depth=4)
# 自适应增强决策树回归模型	
# n_estimators：构建400棵不同权重的决策树，训练模型
model = se.AdaBoostRegressor(model, n_estimators=400, random_state=7)
# 训练模型
model.fit(train_x, train_y)
# 测试模型
pred_test_y = model.predict(test_x)
```

**自助聚合**

每次从总样本矩阵中以有放回抽样的方式随机抽取部分样本构建决策树，这样形成多棵包含不同训练样本的决策树，以削弱某些强势样本对模型预测结果的影响，提高模型的泛化特性。

**随机森林**

在自助聚合的基础上，每次构建决策树模型时，不仅随机选择部分样本，而且还随机选择部分特征，这样的集合算法，不仅规避了强势样本对预测结果的影响，而且也削弱了强势特征的影响，使模型的预测能力更加泛化。

随机森林相关API：

```python
import sklearn.ensemble as se
# 随机森林回归模型	（属于集合算法的一种）
# max_depth：决策树最大深度10
# n_estimators：构建1000棵决策树，训练模型
# min_samples_split: 子表中最小样本数 若小于这个数字，则不再继续向下拆分
model = se.RandomForestRegressor(max_depth=10, n_estimators=1000, min_samples_split=2)
```

#### ⑵、分类

##### ①、二元逻辑分类：

可以把样本数据经过线性预测模型求得的值带入逻辑函数的x，即将预测函数的输出看做输入被划分为1类的概率，择概率大的类别作为预测结果，可以根据函数值确定两个分类。这是线性函数非线性化的一种方式。

```python
import sklearn.linear_model as lm
# 构建逻辑回归器 
# solver：逻辑函数中指数的函数关系（liblinear为线型函数关系）
# C：参数代表正则强度，为了防止过拟合。正则越大拟合效果越小。
model = lm.LogisticRegression(solver='liblinear', C=正则强度)
model.fit(训练输入集，训练输出集)
result = model.predict(带预测输入集)
```

##### ②、朴素贝叶斯分类:

朴素贝叶斯分类是一种依据统计概率理论而实现的一种分类方式

```python
# 创建高斯分布朴素贝叶斯分类器
model = nb.GaussianNB()
model.fit(x, y)
result = model.predict(samples)
```

**数据集划分**：

对于分类问题训练集和测试集的划分不应该用整个样本空间的特定百分比作为训练数据，而应该在其每一个类别的样本中抽取特定百分比作为训练数据。sklearn模块提供了数据集划分相关方法，可以方便的划分训练集与测试集数据，使用不同数据集训练或测试模型，达到提高分类可信度。

```python
import sklearn.model_selection as ms

ms.train_test_split(输入集, 输出集, test_size=测试集占比, random_state=随机种子)
    ->训练输入, 测试输入, 训练输出, 测试输出
```

**交叉验证**：

由于数据集的划分有不确定性，若随机划分的样本正好处于某类特殊样本，则得到的训练模型所预测的结果的可信度将受到质疑。所以需要进行多次交叉验证，把样本空间中的所有样本均分成n份，使用不同的训练集训练模型，对不同的测试集进行测试时输出指标得分。

sklearn提供了交叉验证相关API：

```python
import sklearn.model_selection as ms
ms.cross_val_score(模型, 输入集, 输出集, cv=折叠数, scoring=指标名)->指标值数组
```

举例：

```python
# 划分训练集和测试集
train_x, test_x, train_y, test_y = \
    ms.train_test_split(
        x, y, test_size=0.25, random_state=7)
# 朴素贝叶斯分类器
model = nb.GaussianNB()
# 交叉验证
# 精确度
ac = ms.cross_val_score( model, train_x, train_y, cv=5, scoring='accuracy')
print(ac.mean())
#用训练集训练模型
model.fit(train_x, train_y)
```

交叉验证指标：

```python
# 交叉验证
# 精确度
ac = ms.cross_val_score( model, train_x, train_y, cv=5, scoring='accuracy')
print(ac.mean())
# 查准率
pw = ms.cross_val_score( model, train_x, train_y, cv=5, scoring='precision_weighted')
print(pw.mean())
# 召回率
rw = ms.cross_val_score( model, train_x, train_y, cv=5, scoring='recall_weighted')
print(rw.mean())
# f1得分
fw = ms.cross_val_score( model, train_x, train_y, cv=5, scoring='f1_weighted')
print(fw.mean())
```

**混淆矩阵**:

```python
import sklearn.metrics as sm
sm.confusion_matrix(实际输出, 预测输出)->混淆矩阵
```

```python
#输出混淆矩阵并绘制混淆矩阵图谱
cm = sm.confusion_matrix(test_y, pred_test_y)
print(cm)
mp.figure('Confusion Matrix', facecolor='lightgray')
mp.title('Confusion Matrix', fontsize=20)
mp.xlabel('Predicted Class', fontsize=14)
mp.ylabel('True Class', fontsize=14)
mp.xticks(np.unique(pred_test_y))
mp.yticks(np.unique(test_y))
mp.tick_params(labelsize=10)
mp.imshow(cm, interpolation='nearest', cmap='jet')
mp.show()
```

**分类报告**：

```python
# 获取分类报告
cr = sm.classification_report(实际输出, 预测输出)
```

```python
# 获取分类报告
cr = sm.classification_report(test_y, pred_test_y)
print(cr)
```

##### ③、决策树分类：

```python
sklearn.tree.DecisionTreeClassifier(criterion='gini',max_depth=None,random_state=None)
决策树的分类器
    criterion: 默认是'gini'系数，也可以选择信息增益的熵'entropy'.
        #决策树的划分方式，默认为gini，更加精细化#
    max_depth: 树的深度大小，层
    random_state:随机数种子
    method:
    decision_path:返回决策树的路径
model=st.DecisionTreeClassifier(criterion='entropy',max_depth=6,random_state=7)
model=st.DecisionTreeClassifier(criterion='gini',max_depth=6,random_state=7)
```

##### ④、支持向量机SVM:

> SVM 是一种二分类模型。它的基本思想是在特征空间中寻找间隔最大的分离超平面使数据得到高效的二分类，具体来讲，有三种情况（不加核函数的话就是个线性模型，加了之后才会升级为一个非线性模型）：
>
> - 当训练样本线性可分时，通过**硬间隔**最大化，学习一个线性分类器，即线性可分支持向量机；
> - 当训练数据近似线性可分时，引入**松弛变量**，通过**软间隔**最大化，学习一个线性分类器，即线性支持向量机；
> - 当训练数据线性不可分时，通过使用**核技巧**及软间隔最大化，学习非线性支持向量机。

```python
model = svm.SVC(kernel='linear',class_weight='balanced')  
# kernel的取值：
# {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'}
model.fit(train_x, train_y) 
```

**样本类别均衡化**：

如果遇到样本数量不均衡，如下解决办法：

1）设置类别权重class_weighted，均衡化样本数量的差异。

2）上采样或下采样

通过类别权重的均衡化，使所占比例较小的样本权重较高，而所占比例较大的样本权重较低，以此平均化不同类别样本对分类模型的贡献，提高模型性能。

样本类别均衡化相关API：

```python
model = svm.SVC(kernel='linear', class_weight='balanced')
model.fit(train_x, train_y)
```

**置信概率**：

根据样本与分类边界的距离远近，对其预测类别的可信程度进行量化，离边界越近的样本，置信概率越低，反之，离边界越远的样本，置信概率高。

获取每个样本的置信概率相关API：

```python
# 在获取模型时，给出超参数probability=True
model = svm.SVC(kernel='rbf', C=600, gamma=0.01, probability=True)
预测结果 = model.predict(输入样本矩阵)
# 调用model.predict_proba(样本矩阵)可以获取每个样本的置信概率矩阵
置信概率矩阵 = model.predict_proba(输入样本矩阵)
```

**网格搜索**:

获取一个最优超参数的方式可以绘制验证曲线，但是验证曲线只能每次获取一个最优超参数。如果多个超参数有很多排列组合的话，就可以**使用网格搜索寻求最优超参数组合**。

针对超参数组合列表中的每一个超参数组合，实例化给定的模型，做cv次交叉验证，将其中平均f1得分最高的超参数组合作为最佳选择，实例化模型对象。

网格搜索相关API：

```python
import sklearn.model_selection as ms
model = ms.GridSearchCV(模型, 超参数组合列表, cv=折叠数)
model.fit(输入集，输出集)
# 获取网格搜索每个参数组合
model.cv_results_['params']
# 获取网格搜索每个参数组合所对应的平均测试分值
model.cv_results_['mean_test_score']
# 获取最好的参数
model.best_params_
model.best_score_
model.best_estimator_
```

案例：

```python
# 基于径向基核函数的支持向量机分类器
params = [{'kernel':['linear'], 'C':[1, 10, 100, 1000]},
    {'kernel':['poly'], 'C':[1], 'degree':[2, 3]}, 
    {'kernel':['rbf'], 'C':[1,10,100,1000], 'gamma':[1, 0.1, 0.01, 0.001]}]
model = ms.GridSearchCV(svm.SVC(probability=True), params, cv=5)  # 创建对象
model.fit(train_x, train_y) # 训练数据
for p, s in zip(model.cv_results_['params'],
        model.cv_results_['mean_test_score']):
    print(p, s)
# 获取得分最优的的超参数信息
print(model.best_params_)
# 获取最优得分
print(model.best_score_)
# 获取最优模型的信息
print(model.best_estimator_)
```

#### ⑶、聚类

##### ①、K-Means算法：

算法流程：

第一步：随机选择k个样本作为k个聚类的中心，计算每个样本到各个聚类中心的欧氏距离，将该样本分配到与之距离最近的聚类中心所在的类别中。

第二步：根据第一步所得到的聚类划分，分别计算每个聚类的几何中心，将几何中心作为新的聚类中心，重复第一步，直到计算所得**几何中心**与**聚类中心**重合或接近重合为止。

```python
import sklearn.cluster as sc
# n_clusters: 聚类数
model = sc.KMeans(n_clusters=4)
# 不断调整聚类中心，知道最终聚类中心稳定则聚类完成
model.fit(x)
y = model.predict(x) # 预测x中每个样本的类别标签
y = model.labels_  # 直接返回每一个训练样本的类别标签
# 获取训练结果的聚类中心
centers = model.cluster_centers_
```

##### ②、KNN算法

算法流程：

> 1、计算测试对象到训练集中每个对象的距离
>
> 2、按照距离的远近排序
>
> 3、选取与当前测试对象最近的K个训练对象，作为该测试对象的邻居
>
> 4、统计这K个邻居的类别频次
>
> 5、K个邻居里频次最高的类别，即为测试对象的类别

```python
sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)
# 参数:
	n_neighbors -- 选定参考几个邻居
```



##### ③、均值漂移:

首先假定样本空间中的每个聚类均服从某种已知的概率分布规则，然后用不同的概率密度函数拟合样本中的统计直方图，不断移动密度函数的中心(均值)的位置，直到获得最佳拟合效果为止。这些概率密度函数的峰值点就是聚类的中心，再根据每个样本距离各个中心的距离，选择最近聚类中心所属的类别作为该样本的类别。

均值漂移算法的特点：

1. 聚类数不必事先已知，算法会自动识别出统计直方图的中心数量。
2. 聚类中心不依据于最初假定，聚类划分的结果相对稳定。
3. 样本空间应该服从某种概率分布规则，否则算法的准确性会大打折扣。

均值漂移算法相关API：

```python
# 量化带宽，决定每次调整概率密度函数的步进量
# n_samples：样本数量
# quantile：量化宽度（直方图一条的宽度）
bw = sc.estimate_bandwidth(x, n_samples=len(x), quantile=0.1)
# 均值漂移聚类器
model = sc.MeanShift(bandwidth=bw, bin_seeding=True)
model.fit(x)
```

##### ④、轮廓系数:

好的聚类：内密外疏，同一个聚类内部的样本要足够密集，不同聚类之间样本要足够疏远。

轮廓系数计算规则：针对样本空间中的一个特定样本，计算它与所在聚类其它样本的平均距离a，以及该样本与距离最近的另一个聚类中所有样本的平均距离b，该样本的轮廓系数为(b-a)/max(a, b)，将整个样本空间中所有样本的轮廓系数取算数平均值，作为**聚类划分的性能指标s**。

轮廓系数的区间为：[-1, 1]。 -1代表分类效果差，1代表分类效果好。0代表聚类重叠，没有很好的划分聚类。

轮廓系数相关API：

```python
import sklearn.metrics as sm
# v：平均轮廓系数
# metric：距离算法：使用欧几里得距离(euclidean)
v = sm.silhouette_score(输入集, 输出集, sample_size=样本数, metric=距离算法)
# 案例：
# 打印平均轮廓系数
print(sm.silhouette_score( x, pred_y, sample_size=len(x), metric='euclidean'))
```

##### ⑤、DBSCAN算法：

DBSCAN算法的特点：

1. 事先给定的半径会影响最后的聚类效果，可以借助轮廓系数选择较优的方案。

2. 根据聚类的形成过程，把样本细分为以下三类：

   外周样本：被其它样本聚集到某个聚类中，但无法再引入新样本的样本。

   孤立样本：聚类中的样本数低于所设定的下限，则不称其为聚类，反之称其为孤立样本。

   核心样本：除了外周样本和孤立样本以外的样本。

DBSCAN聚类算法相关API：

```python
# DBSCAN聚类器
# eps：半径
# min_samples：聚类样本数的下限，若低于该数值，则称为孤立样本
model = sc.DBSCAN(eps=epsilon, min_samples=5)
model.fit(x)
y = model.labels_
x[y == -1]的样本则是孤立样本
x[model.core_sample_indices_]为核心样本
```



#### 1、决策树---分类+回归

- 决策树的优点：

  1. 可解释性比较强
  2. 可以处理缺失属性值的样本

- 决策树的缺点：

  1. 容易造成过拟合，需要采用剪枝操作
  2. 忽略了数据之间的相关性

- 构建决策树有3种算法：

  **ID3、C4.5、CART**

- 构建决策树的三步走：

  **特征选择、决策树的生成、决策树的修剪**

  ![](D:\Desktop\8期CV课\项目二(安防监控之实时口罩人脸检测)\images\DicissionTree.jpg)

在划分数据集前后信息发生的变化称为**信息增益**

- **ID3算法**

> 步骤：
>
> 1、初始化特征集合和数据集合；
>
> 2、计算数据集合信息熵和所有特征的条件熵，选择**信息增益**最大的特征作为当前决策节点；
>
> 3、更新数据集合和特征集合(删除上一步使用的特征，并按照特征值来划分不同分支的数据集合)；
>
> 4、重复2/3步骤，若子集包含单一特征，则为分支叶子节点。
>
> 缺点：
>
> 1、ID3没有剪枝策略，容易发生过拟合；
>
> 2、信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于1；
>
> 3、只能用于处理离散分布的特征不能处理连续值；
>
> 4、没有考虑缺失值。

- **C4.5算法**

> - 用**信息增益率**来选择属性，ID3选择属性用的是子树的信息增益；
> - 在决策树构造过程中进行**剪枝**；
> - 对非离散数据也能处理；
> - 能够对不完整数据进行处理。
>
> **C4.5发生过拟合的原因**：
>
> 为了尽可能正确分类样本，节点的划分过程会不断重复直到不能再分，这样就可能对训练样本学习的“太好了”，把训练样本的一些特点当作所有数据都具有额一般性质，从而导致过拟合；
>
> **解决过拟合的方法：**
>
> 通过剪枝处理去掉一部分分支来降低过拟合的风险，剪枝的基本策略有“预剪枝”(prepruning)和“后剪枝”(post-pruning)
>
> **C4.5算法的缺点**：
>
> 1. 用的多叉树，二叉树效率更高；
> 2. 只能用于多分类；
> 3. 使用的熵模型拥有大量耗时的对数运算，连续值还有排序运算。

- **CART树算法**

> - 用基尼指数来选择属性(分类)，活用均方差来选择属性(回归)；
> - 顾名思义，CART算法既可以用于创建分类树，也可以创建回归数，两者在构建的过程中稍有差异；
> - 如果目标变量是离散的，称为分类树；
> - 如果目标变量是连续的，称为回归树。

> ------
>
> 核心思想：相似的输入必会产生相似的输出
>
> ------
>
> 构建决策树算法主要包括三个部分：
>
> 特征选择、树的生成、树的剪枝。常用算法有 **ID3、C4.5、CART**。
>
> - **特征选择**
>
> 特征选择的目的是选取能够对训练集分类的特征。特征选择的关键是准则：信息增益、信息增益比、Gini 指数；
>
> - **决策树的生成**
>
> 通常是利用信息增益最大、信息增益比最大、Gini 指数最小作为特征选择的准则。从根节点开始，递归的生成决策树。相当于是不断选取局部最优特征，或将训练集分割为基本能够正确分类的子集；
>
> - **决策树的剪枝**
>
> 决策树的剪枝是为了防止树的过拟合，增强其泛化能力。包括预剪枝和后剪枝。
> ———————————————————————————————————————————————
>
> https://blog.csdn.net/jiaoyangwm/article/details/79525237
>
> - 决策树代码示例：
>
> ```python
> import sklearn.tree as st
> 
> # 创建决策树回归器模型  决策树的最大深度为4
> model = st.DecisionTreeRegressor(max_depth=4)
> # 训练模型  
> # train_x： 二维数组样本数据
> # train_y： 训练集中对应每行样本的结果
> model.fit(train_x, train_y)
> # 测试模型
> pred_test_y = model.predict(test_x)
> ```
>
> ------
>
>
>
> ![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_01.jpg)
>
> ![07机器学习-决策树_02](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_02.jpg)
>
> ![07机器学习-决策树_03](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_03.jpg)
>
> ![07机器学习-决策树_04](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_04.jpg)
>
> ![07机器学习-决策树_05](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_05.jpg)
>
> ![07机器学习-决策树_06](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_06.jpg)
>
> ![07机器学习-决策树_07](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_07.jpg)
>
> ![07机器学习-决策树_08](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_08.jpg)
>
> ![07机器学习-决策树_09](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_09.jpg)
>
> ![07机器学习-决策树_10](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_10.jpg)
>
> ![07机器学习-决策树_11](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_11.jpg)
>
> ![07机器学习-决策树_12](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_12.jpg)
>
> ![07机器学习-决策树_13](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_13.jpg)
>
> ![07机器学习-决策树_14](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_14.jpg)
>
> ![07机器学习-决策树_15](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_15.jpg)
>
> ![07机器学习-决策树_16](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_16.jpg)
>
> ![07机器学习-决策树_17](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_17.jpg)
>
> ![07机器学习-决策树_18](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_18.jpg)
>
> ![07机器学习-决策树_19](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_19.jpg)
>
> ![07机器学习-决策树_20](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_20.jpg)
>
> ![07机器学习-决策树_21](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_21.jpg)
>
> ![07机器学习-决策树_22](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_22.jpg)
>
> ![07机器学习-决策树_23](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_23.jpg)
>
> ![07机器学习-决策树_24](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_24.jpg)
>
> ![07机器学习-决策树_25](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_25.jpg)
>
> ![07机器学习-决策树_26](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_26.jpg)
>
> ![07机器学习-决策树_27](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_27.jpg)
>
> ![07机器学习-决策树_28](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_28.jpg)
>
> ![07机器学习-决策树_29](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_29.jpg)
>
> ![07机器学习-决策树_30](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_30.jpg)
>
> ![07机器学习-决策树_31](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_31.jpg)
>
> ![07机器学习-决策树_32](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_32.jpg)
>
> ![07机器学习-决策树_33](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_33.jpg)
>
> ![07机器学习-决策树_34](D:\06_机器学习课程\黄海广机器学习PPT\ppt\07机器学习-决策树\07机器学习-决策树_34.jpg)

#### 2、SVM

> SVM 是一种二分类模型。它的基本思想是在特征空间中寻找间隔最大的分离超平面使数据得到高效的二分类，具体来讲，有三种情况（不加核函数的话就是个线性模型，加了之后才会升级为一个非线性模型）：
>
> - 当训练样本线性可分时，通过**硬间隔**最大化，学习一个线性分类器，即线性可分支持向量机；
> - 当训练数据近似线性可分时，引入**松弛变量**，通过**软间隔**最大化，学习一个线性分类器，即线性支持向量机；
> - 当训练数据线性不可分时，通过使用**核技巧**及软间隔最大化，学习非线性支持向量机。
>
> https://zhuanlan.zhihu.com/p/77750026
>
> 基于线性核函数的SVM分类相关API：
>
> ```python
> model = svm.SVC(kernel='linear')
> model.fit(train_x, train_y)
> ```
>
> ```python
> # 基于线性核函数的支持向量机分类器  # poly多项式核函数
> model = svm.SVC(kernel='poly', degree=3)
> model.fit(train_x, train_y)
> ```
>
> ```python
> # 基于径向基核函数的支持向量机分类器
> # C：正则强度
> # gamma：正态分布曲线的标准差
> model = svm.SVC(kernel='rbf', C=600, gamma=0.01)
> model.fit(train_x, train_y)
> ```
>
> 如果遇到样本数量不均衡，如下解决办法：
>
> 1）设置类别权重class_weighted，均衡化样本数量的差异。
>
> 2）上采样或下采样
>
> 通过类别权重的均衡化，使所占比例较小的样本权重较高，而所占比例较大的样本权重较低，以此平均化不同类别样本对分类模型的贡献，提高模型性能。
>
> 样本类别均衡化相关API：
>
> ```python
> model = svm.SVC(kernel='linear', class_weight='balanced')
> model.fit(train_x, train_y)
> ```
>
> ------
>
>

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_02.jpg)

![09机器学习-支持向量机_03](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_03.jpg)

![09机器学习-支持向量机_04](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_04.jpg)

![09机器学习-支持向量机_05](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_05.jpg)

![09机器学习-支持向量机_06](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_06.jpg)

![09机器学习-支持向量机_07](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_07.jpg)

![09机器学习-支持向量机_08](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_08.jpg)

![09机器学习-支持向量机_09](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_09.jpg)

![09机器学习-支持向量机_10](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_10.jpg)

![09机器学习-支持向量机_11](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_11.jpg)

![09机器学习-支持向量机_12](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_12.jpg)

![09机器学习-支持向量机_13](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_13.jpg)

![09机器学习-支持向量机_14](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_14.jpg)

![09机器学习-支持向量机_15](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_15.jpg)

![09机器学习-支持向量机_16](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_16.jpg)

![09机器学习-支持向量机_17](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_17.jpg)

![09机器学习-支持向量机_18](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_18.jpg)

![09机器学习-支持向量机_19](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_19.jpg)

![09机器学习-支持向量机_20](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_20.jpg)

![09机器学习-支持向量机_21](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_21.jpg)

![09机器学习-支持向量机_22](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_22.jpg)

![09机器学习-支持向量机_23](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_23.jpg)

![09机器学习-支持向量机_24](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_24.jpg)

![09机器学习-支持向量机_25](D:\06_机器学习课程\黄海广机器学习PPT\ppt\09机器学习-支持向量机\09机器学习-支持向量机_25.jpg)

#### 3、KNN---分类+回归

核心知识：

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_10.jpg)

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_11.jpg)

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_12.jpg)

------

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_01.jpg)

![06机器学习-KNN算法_02](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_02.jpg)

![06机器学习-KNN算法_03](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_03.jpg)

![06机器学习-KNN算法_04](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_04.jpg)

![06机器学习-KNN算法_05](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_05.jpg)

![06机器学习-KNN算法_06](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_06.jpg)

![06机器学习-KNN算法_07](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_07.jpg)

![06机器学习-KNN算法_08](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_08.jpg)

![06机器学习-KNN算法_09](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_09.jpg)

![06机器学习-KNN算法_10](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_10.jpg)

![06机器学习-KNN算法_11](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_11.jpg)

![06机器学习-KNN算法_12](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_12.jpg)

![06机器学习-KNN算法_13](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_13.jpg)

![06机器学习-KNN算法_14](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_14.jpg)

![06机器学习-KNN算法_15](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_15.jpg)

![06机器学习-KNN算法_16](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_16.jpg)

![06机器学习-KNN算法_17](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_17.jpg)

![06机器学习-KNN算法_18](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_18.jpg)

![06机器学习-KNN算法_19](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_19.jpg)

![06机器学习-KNN算法_20](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_20.jpg)

![06机器学习-KNN算法_21](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_21.jpg)

![06机器学习-KNN算法_22](D:\06_机器学习课程\黄海广机器学习PPT\ppt\06机器学习-KNN算法\06机器学习-KNN算法_22.jpg)

#### 4、聚类

核心：K-means聚类

聚类的评价指标：**轮廓系数**

好的聚类：**内密外疏**，同一个聚类内部的样本要足够密集，不同聚类之间样本要足够疏远。

轮廓系数计算规则：针对样本空间中的一个特定样本，计算它与所在聚类其它样本的平均距离a，以及该样本与距离最近的另一个聚类中所有样本的平均距离b，该样本的轮廓系数为(b-a)/max(a, b)，将整个样本空间中所有样本的轮廓系数取算数平均值，作为聚类划分的性能指标s。

轮廓系数的区间为：[-1, 1]。 -1代表分类效果差，1代表分类效果好。0代表聚类重叠，没有很好的划分聚类。

轮廓系数相关API：

```python
import sklearn.metrics as sm
# v：平均轮廓系数
# metric：距离算法：使用欧几里得距离(euclidean)
v = sm.silhouette_score(输入集, 输出集, sample_size=样本数, metric=距离算法)
```

```python
# 输出KMeans算法聚类划分后的轮廓系数。
# 打印平均轮廓系数
print(sm.silhouette_score( x, pred_y, sample_size=len(x), metric='euclidean'))
```



![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_14.jpg)

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_15.jpg)

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_16.jpg)

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_17.jpg)

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_21.jpg)

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_22.jpg)

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_23.jpg)

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_24.jpg)

------

**正式课件：**

![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_01.jpg)

![10机器学习-聚类_02](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_02.jpg)

![10机器学习-聚类_03](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_03.jpg)

![10机器学习-聚类_04](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_04.jpg)

![10机器学习-聚类_05](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_05.jpg)

![10机器学习-聚类_06](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_06.jpg)

![10机器学习-聚类_07](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_07.jpg)

![10机器学习-聚类_08](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_08.jpg)

![10机器学习-聚类_09](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_09.jpg)

![10机器学习-聚类_10](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_10.jpg)

![10机器学习-聚类_11](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_11.jpg)

![10机器学习-聚类_12](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_12.jpg)

![10机器学习-聚类_13](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_13.jpg)

![10机器学习-聚类_14](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_14.jpg)

![10机器学习-聚类_15](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_15.jpg)

![10机器学习-聚类_16](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_16.jpg)

![10机器学习-聚类_17](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_17.jpg)

![10机器学习-聚类_18](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_18.jpg)

![10机器学习-聚类_19](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_19.jpg)

![10机器学习-聚类_20](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_20.jpg)

![10机器学习-聚类_21](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_21.jpg)

![10机器学习-聚类_22](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_22.jpg)

![10机器学习-聚类_23](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_23.jpg)

![10机器学习-聚类_24](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_24.jpg)

![10机器学习-聚类_25](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_25.jpg)

![10机器学习-聚类_26](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_26.jpg)

![10机器学习-聚类_27](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_27.jpg)

![10机器学习-聚类_28](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_28.jpg)

![10机器学习-聚类_29](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_29.jpg)

![10机器学习-聚类_30](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_30.jpg)

![10机器学习-聚类_31](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_31.jpg)

![10机器学习-聚类_32](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_32.jpg)

![10机器学习-聚类_33](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_33.jpg)

![10机器学习-聚类_34](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_34.jpg)

![10机器学习-聚类_35](D:\06_机器学习课程\黄海广机器学习PPT\ppt\10机器学习-聚类\10机器学习-聚类_35.jpg)

#### K-Means：

> 1、K-Means
>
> 第一步：随机选择k个样本作为k个聚类的中心，计算每个样本到各个聚类中心的欧氏距离，将该样本分配到与之距离最近的聚类中心所在的类别中。
>
> 第二步：根据第一步所得到的聚类划分，分别计算每个聚类的几何中心，将几何中心作为新的聚类中心，重复第一步，直到计算所得**几何中心**与**聚类中心**重合或接近重合为止。
>
> ```python
> K-均值是一个迭代算法，假设我们想要将数据聚类成 n 个组，其方法为:
> 首先选择𝐾个随机的点，称为聚类中心（cluster centroids）；
> 对于数据集中的每一个数据，按照距离𝐾个中心点的距离，将其与距离最近的中心点关
> 联起来，与同一个中心点关联的所有点聚成一类。
> 计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。
> ```
>
>
>
> 分类（class）与聚类（cluster）不同，分类是有监督学习模型，聚类属于无监督学习模型。聚类讲究使用一些算法把样本划分为n个群落。一般情况下，这种算法都需要计算**欧氏距离**。
> $$
> P(x_1) - Q(x_2): |x_1-x_2| = \sqrt{(x_1-x_2)^2} \\
> P(x_1,y_1) - Q(x_2,y_2): \sqrt{(x_1-x_2)^2+(y_1-y_2)^2} \\
> P(x_1,y_1,z_1) - Q(x_2,y_2,z_2): \sqrt{(x_1-x_2)^2+(y_1-y_2)^2+(z_1-z_2)^2} \\
> $$
> K-Means缺点：结果受初始化影响比较大，计算的时间复杂度比较大。
>
> - K均值算法相关API：
>
> ```python
> import sklearn.cluster as sc
> # n_clusters: 聚类数
> model = sc.KMeans(n_clusters=4)
> # 不断调整聚类中心，知道最终聚类中心稳定则聚类完成
> model.fit(x)
> y = model.predict(x) # 预测x中每个样本的类别标签
> y = model.labels_  # 直接返回每一个训练样本的类别标签
> # 获取训练结果的聚类中心
> centers = model.cluster_centers_
> ```
>
> 使用案例：通过K-Means聚类得到anchor：
>
> ```python
> import numpy as np
> 
> def wh_iou(wh1, wh2):
>     # Returns the nxm IoU matrix. wh1 is nx2, wh2 is mx2
>     wh1 = wh1[:, None]  # [N,1,2]
>     wh2 = wh2[None]  # [1,M,2]
>     inter = np.minimum(wh1, wh2).prod(2)  # [N,M]       # 求最小值并作乘积运算
>     return inter / (wh1.prod(2) + wh2.prod(2) - inter)  # iou = inter / (area1 + area2 - inter)
> 
> 
> def k_means(boxes, k, dist=np.median):
>     """
>     yolo k-means methods
>     refer: https://github.com/qqwweee/keras-yolo3/blob/master/kmeans.py
>     Args:
>         boxes: 需要聚类的bboxes
>         k: 簇数(聚成几类)
>         dist: 更新簇坐标的方法(默认使用中位数，比均值效果略好)
>     """
>     box_number = boxes.shape[0]
>     last_nearest = np.zeros((box_number,))
>     # np.random.seed(0)  # 固定随机数种子
> 
>     # init k clusters
>     clusters = boxes[np.random.choice(box_number, k, replace=False)]  # 随机选择k个bbox作为簇心 # replace=Fa表示不会有重复lse
> 
>     while True:
>         distances = 1 - wh_iou(boxes, clusters)
>         current_nearest = np.argmin(distances, axis=1)  # 返回离的最近的anchor的索引
>         if (last_nearest == current_nearest).all():
>             break  # clusters won't change
>         for cluster in range(k):
>             # update clusters簇心
>             clusters[cluster] = dist(boxes[current_nearest == cluster], axis=0)
> 
>         last_nearest = current_nearest
> 
>     return clusters
> 
> ```

#### 5、线性回归

> ![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_01.jpg)
>
> ![02机器学习-回归_02](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_02.jpg)
>
> ![02机器学习-回归_03](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_03.jpg)
>
> ![02机器学习-回归_04](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_04.jpg)
>
> ![02机器学习-回归_05](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_05.jpg)
>
> ![02机器学习-回归_06](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_06.jpg)
>
> ![02机器学习-回归_07](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_07.jpg)
>
> ![02机器学习-回归_08](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_08.jpg)
>
> ![02机器学习-回归_09](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_09.jpg)
>
> ![02机器学习-回归_10](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_10.jpg)
>
> ![02机器学习-回归_11](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_11.jpg)
>
> ![02机器学习-回归_12](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_12.jpg)
>
> ![02机器学习-回归_13](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_13.jpg)
>
> ![02机器学习-回归_14](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_14.jpg)
>
> ![02机器学习-回归_15](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_15.jpg)
>
> ![02机器学习-回归_16](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_16.jpg)
>
> ![02机器学习-回归_17](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_17.jpg)
>
> ![02机器学习-回归_18](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_18.jpg)
>
> ![02机器学习-回归_19](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_19.jpg)
>
> ![02机器学习-回归_20](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_20.jpg)
>
> ![02机器学习-回归_21](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_21.jpg)
>
> ![02机器学习-回归_22](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_22.jpg)
>
> ![02机器学习-回归_23](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_23.jpg)
>
> ![02机器学习-回归_24](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_24.jpg)
>
> ![02机器学习-回归_25](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_25.jpg)
>
> ![02机器学习-回归_26](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_26.jpg)
>
> ![02机器学习-回归_27](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_27.jpg)
>
> ![02机器学习-回归_28](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_28.jpg)
>
> ![02机器学习-回归_29](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_29.jpg)
>
> ![02机器学习-回归_30](D:\06_机器学习课程\黄海广机器学习PPT\ppt\02机器学习-回归\02机器学习-回归_30.jpg)

#### 6、逻辑回归--二元分类

前言：

1）什么是二元分类

**二元分类又称逻辑回归**，是将一组样本划分到两个不同类别的分类方式。

2）如何实现二元分类

逻辑回归属于广义线性回归模型（generalized linear model），使用线性模型计算函数值，再通过逻辑函数将连续值进行离散化处理。逻辑函数（又称sigmoid函数）表达式为：
$$
z=1/(1+e^-y)，其中y=ax+b
$$
该函数能将y值压缩到(0, 1)区间，通过选取合适的阈值转换为两个离散值（大于0.5为1，小于0.5为0）。

> ![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_01.jpg)
>
> ![03机器学习-逻辑回归_02](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_02.jpg)
>
> ![03机器学习-逻辑回归_03](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_03.jpg)
>
> ![03机器学习-逻辑回归_04](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_04.jpg)
>
> ![03机器学习-逻辑回归_05](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_05.jpg)
>
> ![03机器学习-逻辑回归_06](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_06.jpg)
>
> ![03机器学习-逻辑回归_07](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_07.jpg)
>
> ![03机器学习-逻辑回归_08](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_08.jpg)
>
> ![03机器学习-逻辑回归_09](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_09.jpg)
>
> ![03机器学习-逻辑回归_10](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_10.jpg)
>
> ![03机器学习-逻辑回归_11](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_11.jpg)
>
> ![03机器学习-逻辑回归_12](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_12.jpg)
>
> ![03机器学习-逻辑回归_13](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_13.jpg)
>
> ![03机器学习-逻辑回归_14](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_14.jpg)
>
> ![03机器学习-逻辑回归_15](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_15.jpg)
>
> ![03机器学习-逻辑回归_16](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_16.jpg)
>
> ![03机器学习-逻辑回归_17](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_17.jpg)
>
> ![03机器学习-逻辑回归_18](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_18.jpg)
>
> ![03机器学习-逻辑回归_19](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_19.jpg)
>
> ![03机器学习-逻辑回归_20](D:\06_机器学习课程\黄海广机器学习PPT\ppt\03机器学习-逻辑回归\03机器学习-逻辑回归_20.jpg)

#### 7、朴素贝叶斯

> 贝叶斯定理：P(A|B)=P(B|A)P(A)/P(B)      <==     *P(A, B) = P(A) P(B|A) = P(B) P(A|B)*
>
> ![](D:\Desktop\8期CV课\项目二(安防监控之实时口罩人脸检测)\images\朴素贝叶斯定理.jpg)
>
> https://blog.csdn.net/sinat_30353259/article/details/80932111
>
> ![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_01.jpg)
>
> ![04机器学习-朴素贝叶斯_02](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_02.jpg)
>
> ![04机器学习-朴素贝叶斯_03](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_03.jpg)
>
> ![04机器学习-朴素贝叶斯_04](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_04.jpg)
>
> ![04机器学习-朴素贝叶斯_05](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_05.jpg)
>
> ![04机器学习-朴素贝叶斯_06](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_06.jpg)
>
> ![04机器学习-朴素贝叶斯_07](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_07.jpg)
>
> ![04机器学习-朴素贝叶斯_08](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_08.jpg)
>
> ![04机器学习-朴素贝叶斯_09](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_09.jpg)
>
> ![04机器学习-朴素贝叶斯_10](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_10.jpg)
>
> ![04机器学习-朴素贝叶斯_11](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_11.jpg)
>
> ![04机器学习-朴素贝叶斯_12](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_12.jpg)
>
> ![04机器学习-朴素贝叶斯_13](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_13.jpg)
>
> ![04机器学习-朴素贝叶斯_14](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_14.jpg)
>
> ![04机器学习-朴素贝叶斯_15](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_15.jpg)
>
> ![04机器学习-朴素贝叶斯_16](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_16.jpg)
>
> ![04机器学习-朴素贝叶斯_17](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_17.jpg)
>
> ![04机器学习-朴素贝叶斯_18](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_18.jpg)
>
> ![04机器学习-朴素贝叶斯_19](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_19.jpg)
>
> ![04机器学习-朴素贝叶斯_20](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_20.jpg)
>
> ![04机器学习-朴素贝叶斯_21](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_21.jpg)
>
> ![04机器学习-朴素贝叶斯_22](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_22.jpg)
>
> ![04机器学习-朴素贝叶斯_23](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_23.jpg)
>
> ![04机器学习-朴素贝叶斯_24](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_24.jpg)
>
> ![04机器学习-朴素贝叶斯_25](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_25.jpg)
>
> ![04机器学习-朴素贝叶斯_26](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_26.jpg)
>
> ![04机器学习-朴素贝叶斯_27](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_27.jpg)
>
> ![04机器学习-朴素贝叶斯_28](D:\06_机器学习课程\黄海广机器学习PPT\ppt\04机器学习-朴素贝叶斯\04机器学习-朴素贝叶斯_28.jpg)

#### 补充知识：Scikit-learn

> ![](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_01.jpg)
>
> ![Scikit-learn_02](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_02.jpg)
>
> ![Scikit-learn_03](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_03.jpg)
>
> ![Scikit-learn_04](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_04.jpg)
>
> ![Scikit-learn_05](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_05.jpg)
>
> ![Scikit-learn_06](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_06.jpg)
>
> ![Scikit-learn_07](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_07.jpg)
>
> ![Scikit-learn_08](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_08.jpg)
>
> ![Scikit-learn_09](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_09.jpg)
>
> ![Scikit-learn_10](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_10.jpg)
>
> ![Scikit-learn_11](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_11.jpg)
>
> ![Scikit-learn_12](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_12.jpg)
>
> ![Scikit-learn_13](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_13.jpg)
>
> ![Scikit-learn_14](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_14.jpg)
>
> ![Scikit-learn_15](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_15.jpg)
>
> ![Scikit-learn_16](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_16.jpg)
>
> ![Scikit-learn_17](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_17.jpg)
>
> ![Scikit-learn_18](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_18.jpg)
>
> ![Scikit-learn_19](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_19.jpg)
>
> ![Scikit-learn_20](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_20.jpg)
>
> ![Scikit-learn_21](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_21.jpg)
>
> ![Scikit-learn_22](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_22.jpg)
>
> ![Scikit-learn_23](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_23.jpg)
>
> ![Scikit-learn_24](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_24.jpg)
>
> ![Scikit-learn_25](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_25.jpg)
>
> ![Scikit-learn_26](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_26.jpg)
>
> ![Scikit-learn_27](D:\06_机器学习课程\黄海广机器学习PPT\ppt\Scikit-learn\Scikit-learn_27.jpg)

### 机器学习评价指标