















## 深度学习总结

![img](https://i0.hdslb.com/bfs/article/bd44ca98db13d22d90ecbbeb4aba29adf7c7ce47.png@942w_572h_progressive.webp)

## 1、深度学习常见评价指标

#### 1、回归（Regression）算法指标

- ##### Mean Absolute Error 平均绝对误差

平均绝对误差MAE（Mean Absolute Error）又被称为 L1范数损失。

![](https://pic4.zhimg.com/80/v2-8e51113f9fd7600d751bb98324be601b_720w.jpg)

- ##### Mean Squared Error 均方误差

均方误差MSE（Mean Squared Error）又被称为 L2范数损失 。

![img](https://pic4.zhimg.com/80/v2-8f9e7eb5813eb1948b1aed75ad555f23_720w.jpg)

- Root Mean Squared Error：均方根误差
- Coefficient of determination 决定系数

#### 2、分类（Classification）算法指标

- 精度 Accuracy
- 混淆矩阵 Confusion Matrix
- 准确率（查准率） Precision
- 召回率（查全率）Recall
- Fβ Score
- AUC Area Under Curve
- KS Kolmogorov-Smirnov

##### **精度**

- Accuracy

定义：(TP+TN)/(TP+FN+FP+TN)   对角线和/总和
 即**所有分类正确的样本占全部样本的比例**

预测正确的样本的占总样本的比例，取值范围为[0,1]，取值越大，模型预测能力越好。

![img](https://pic4.zhimg.com/80/v2-ca1ffae2aea7186d5bf2d1b429cda36b_720w.jpg)

![img](https://pic3.zhimg.com/80/v2-a6cded3a79bea0fff0f1b1d31f26b362_720w.jpg)

##### 准确率

- Precision、查准率---------------分类准不准

定义：(TP)/(TP+FP)    TP/负对角线之和
 即**预测是正例的结果中，确实是正例的比例**

Precision 是分类器预测的正样本中预测正确的比例，取值范围为[0,1]，取值越大，模型预测能力越好。

![img](https://pic2.zhimg.com/80/v2-940da61a950db52d0cb8eac93954e5f5_720w.jpg)

##### 召回率

- Recall、查全率---------------分类全不全

定义：(TP)/(TP+FN)
 即**所有正例的样本中，被找出的比例**

Recall 是分类器所预测正确的正样本占所有正样本的比例，取值范围为[0,1]，取值越大，模型预测能力越好。

![img](https://pic2.zhimg.com/80/v2-1645d7a375ed7bc6a841832306aca6d1_720w.jpg)

混淆矩阵如下：

- True positive (TP)

  真实值为Positive，预测正确（预测值为Positive）

- True negative (TN)

  真实值为Negative，预测正确（预测值为Negative）

- False positive (FP)

  真实值为Negative，预测错误（预测值为Positive），第一类错误， Type I error。

- False negative (FN)

  真实值为Positive，预测错误（预测值为 Negative），第二类错误， Type II error。

![](images\机器学习评价指标.jpg)

##### Fβ Score

Precision和Recall 是互相影响的，理想情况下肯定是做到两者都高，但是一般情况下Precision高、Recall 就低， Recall 高、Precision就低。为了均衡两个指标，我们可以采用Precision和Recall的加权调和平均（weighted harmonic mean）来衡量，即Fβ Score，公式如下：

![img](https://pic2.zhimg.com/80/v2-912ef3ef025cc30bfd2081d5515492a5_720w.jpg)

其中β表示权重：

​	![img](https://pic3.zhimg.com/80/v2-17a3b46890e2d64d70e355ba274a0372_720w.jpg)

通俗的语言就是：β 越大，Recall的权重越大， 越小，Precision的权重越大。

由于Fβ Score 无法直观反映数据的情况，同时业务含义相对较弱，实际工作用到的不多。

##### ROC 和 AUC

AUC是一种模型分类指标，且仅仅是二分类模型的评价指标。AUC是Area Under Curve的简称，那么Curve就是 ROC（Receiver Operating Characteristic），翻译为"接受者操作特性曲线"。也就是说**ROC是一条曲线，AUC是 一个面积值**。

AUC的作用：

https://zhuanlan.zhihu.com/p/121760341

ROC曲线为 **FPR 与 TPR 之间的关系曲线**，这个组合以 FPR 对 TPR，即是以代价 (costs) 对收益 (benefits)，显然收益越高，代价越低，模型的性能就越好。

- x 轴为假阳性率（FPR）：在所有的负样本中，分类器预测错误的比例

![img](https://pic1.zhimg.com/80/v2-0a9da21100f47c508d84c94980ffe40c_720w.jpg)

- y 轴为真阳性率（TPR）：在所有的正样本中，分类器预测正确的比例（等于Recall）

![img](https://pic1.zhimg.com/80/v2-6aa6e7d8c9688bcdbe07acc116255158_720w.jpg)

![img](https://pic2.zhimg.com/80/v2-9edc860c849ab1737d76d7ee138b31cd_720w.jpg)

**AUC**

AUC定义：

- AUC 值为 ROC 曲线所覆盖的**区域面积**，显然，AUC越大，分类器分类效果越好。
- AUC = 1，是完美分类器。
- 0.5 < AUC < 1，优于随机猜测。有预测价值。
- AUC = 0.5，跟随机猜测一样（例：丢铜板），没有预测价值。
- AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

*注：对于AUC小于 0.5 的模型，我们可以考虑取反（模型预测为positive，那我们就取negtive），这样就可以保证模型的性能不可能比随机猜测差。*

以下为ROC曲线和AUC值的实例：

![img](https://pic4.zhimg.com/80/v2-18db5822a9accd275adf89ca3fa2d3cf_720w.jpg)



详细请看下面链接博客：

https://www.jianshu.com/p/b425f5d9fae0

https://zhuanlan.zhihu.com/p/36305931

![](images\机器学习评价指标.jpg)

#### 3、聚类的评价指标-轮廓系数

```python
import sklearn.metrics as sm
# v：平均轮廓系数
# metric：距离算法：使用欧几里得距离(euclidean)
v = sm.silhouette_score(输入集, 输出集, sample_size=样本数, metric=距离算法)
```



#### SIFT

SIFT（Scale-invariant feature transform)

> https://blog.csdn.net/abcjennifer/article/details/7639681

#### HOG：

方向梯度直方图（Histogram of Oriented Gradient, HOG）

> https://blog.csdn.net/zouxy09/article/details/7929348

#### **numpy**

> 见学习笔记

#### **pandas**

> 见学习笔记

## 2、深度学习基础概念

每个模型要掌握到这种程度：原理和结构必须能说出来，而且每个算法的优缺点也能说出来；

卷积计算公式：

**O=(I+2P-K)/S+1**

#### 2.1 欠拟合与过拟合

##### 1）什么是欠拟合、过拟合？

欠拟合指的是模型在训练和预测时表现都不好的情况；过拟合是指模型对于训练数据拟合呈过当的情况，反映到评估指标上，就是模型在训练集上的表现很好，但在测试集和新数据上的表现较差。

##### 2）如何避免过拟合？

- **扩大样本数据**。使用更多的训练数据是解决过拟合问题最有效的手段，因为更多的样本能够让模型学习到更多更有效的特征，减小噪声的影响。
- **降低模型复杂度**。在数据较少时，模型过于复杂是产生过拟合的主要因素，适当降低模型复杂度可以避免模型拟合过多的采样噪声。例如，在神经网络模型中**减少网络层数、神经元个数**等；在决策树模型中降低树的深度、进行剪枝等。
- **正则化方法**。给模型的参数加上一定的正则约束，比如将权值的大小加入到损失函数中。
- 集成学习方法。集成学习是把多个模型集成在一起，来降低单一模型的过拟合风险。
- 树结构中，对树进行枝剪。
- 减少训练迭代次数。
- **神经网络中，加入dropout操作层**。
- 主动加入噪声数据样本。

##### 3）如何避免欠拟合？

- **添加新特征**。当特征不足或者现有特征与样本标签的相关性不强时，模型容易出现欠拟合。
- **增加模型复杂度**。简单模型的学习能力较差，通过增加模型的复杂度可以使模型拥有更强的拟合能力。例如，在线性模型中添加高次项，在神经网络模型中增加网络层数或神经元个数等。
- **减小正则化系数**。正则化是用来防止过拟合的，但当模型出现欠拟合现象时，则需要有针对性地减小正则化系数。

#### 2.2 回归问题模型评估指标？

- 平均绝对误差（Mean Absolute Deviation）：单个观测值与算术平均值的偏差的绝对值的平均。
- 均方误差：单个样本到平均值差值的平方平均值。
- MAD(中位数绝对偏差)：与数据中值绝对偏差的中值。
- R2决定系数：趋向于1，模型越好；趋向于0，模型越差。

#### 2.3 分类问题模型评估指标？

查准率（Precision）：分类正确数量 / (分类正确数量 + 多分类的数量)，范围[0,1]。简单说来就是“分类对不对”。

召回率（Precision）：分类正确数量 / (分类正确数量 + 少分类的数量)，范围[0,1]。简单说来就是“分类全不全”。

F1得分：2 * 查询率 * 召回率 / (查准率 + 召回率), 范围[0, 1] ，综合考虑了查准率、召回率。

#### 2.4 什么是置信概率？

置信概率（Confidence Probability）是用来衡量推断靠程度的概率，该值越大说明推断结果确定性越大，该值越小说明推断结果不确定性越大。

#### 2.5 什么是前馈神经网络

前馈神经网络（feedforward neural network）又称多层感知机（multilayer perceptron, MLP），是典型的深度学习模型。它是一种单向多层结构，其中每一层包含若干个神经元。在此种神经网络中，各神经元可以接收前一层神经元的信号，并产生输出到下一层。第0层叫**输入层**，最后一层叫**输出层**，其他中间层叫做隐含层（或隐藏层、隐层），隐含层可以是一层，也可以是多层。整个网络中无反馈，信号从输入层向输出层**单向传播**，可用一个有向无环图表示。

#### 2.6 什么是激活函数，为什么要使用激活函数

激活函数（activation function），指神经网络中将输入信号的总和转换为输出信号的函数，激活函数将多层感知机输出转换为非线性，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以应用到众多的非线性模型中。

#### 2.7 神经网络中常用的激活函数有哪些，各自有什么特点

1）sigmoid

① 定义：sigmoid函数也叫Logistic函数，用于隐层神经元输出，能将的数值映射到(0,1)的区间，可以用来做二分类。表达式为：

![img](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg-blog.csdnimg.cn%2F2020120319504175.png%3Fx-oss-process%3Dimage%2Fwatermark%2Ctype_ZmFuZ3poZW5naGVpdGk%2Cshadow_10%2Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L1FBUV8wdjA%3D%2Csize_16%2Ccolor_FFFFFF%2Ct_70&refer=http%3A%2F%2Fimg-blog.csdnimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1659656668&t=28c1dbcfa2c131a6cd172caafa5ae8c9)

logistic逻辑回归函数：

![img](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fwww.pianshen.com%2Fimages%2F802%2F1233ca12e5a6390ee704475d23cd65a2.png&refer=http%3A%2F%2Fwww.pianshen.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1659657027&t=130fdac06ffdbb58e36a058383f666cd)

② 特点

- 优点：平滑、易于求导
- 缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现梯度消失

2）tanh

① 定义：双曲正切函数，表达式为：

![img](https://upload-images.jianshu.io/upload_images/20446214-dc245405736c3937.png?imageMogr2/auto-orient/strip|imageView2/2/w/1199/format/webp)

![tanh.png](https://img-blog.csdnimg.cn/img_convert/dfe170bf0568f3be2b531a883d25dca5.png)

② 特点

- 优点：平滑、易于求导；输出均值为0，收敛速度要比sigmoid快，从而可以减少迭代次数
- 缺点：很容易就会出现梯度消失

3）relu

① 定义：修正线性单元，其表达式为：

![img](https://img-blog.csdnimg.cn/img_convert/a669df77f73be3e07e46b420fdbf7954.png)

![relu.png](https://img-blog.csdnimg.cn/img_convert/8fdf8e2b30ae90676f9879b7056839b4.png)

② 特点：

- 优点：计算过程简单；避免了梯度爆炸和梯度消失问题
- 缺点：小于等于0时无输出

4）softmax

① 定义：其表达式为：

![img](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg-blog.csdnimg.cn%2F202101082127457.png%3Fx-oss-process%3Dimage%2Fwatermark%2Ctype_ZmFuZ3poZW5naGVpdGk%2Cshadow_10%2Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ3NzU3MDc5%2Csize_16%2Ccolor_FFFFFF%2Ct_70%23pic_center&refer=http%3A%2F%2Fimg-blog.csdnimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1659657155&t=779c077e8bbf904485da0ce7c75069d9)

![img](https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fimg-blog.csdnimg.cn%2F2021051111041170.png%3Fx-oss-process%3Dimage%2Fwatermark%2Ctype_ZmFuZ3poZW5naGVpdGk%2Cshadow_10%2Ctext_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1MDA4ODMx%2Csize_16%2Ccolor_FFFFFF%2Ct_70&refer=http%3A%2F%2Fimg-blog.csdnimg.cn&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=auto?sec=1659657155&t=a507bf2cf5a867278a3df6da5dba7558)

#### 2.8 什么是softmax函数，其主要作用是什么

1）定义：Softmax函数就可以将多分类的输出数值转化为相对概率，而这些值的累和为1。表达式为：

其中 是分类器前级输出单元的输出。i 表示类别索引，总的类别个数为 C。表示的是当前元素的指数与所有元素指数和的比值。

2）作用：softmax一般用于分类输出层，计算属于每个类别的概率。

#### 2.9 什么是损失函数，损失函数的作用是什么

损失函数（Loss Function），也有称之为代价函数（Cost Function），用来度量预测值和实际值之间的差异，从而作为模型性能参考依据。损失函数值越小，说明预测输出和实际结果（也称期望输出）之间的差值就越小，也就说明我们构建的模型越好，反之说明模型越差。

#### 2.10 什么是交叉熵，其作用是什么

交叉熵（Cross Entropy）主要用于度量两个概率分布间的差异性信息，在机器学习中用来作为分类问题的损失函数。当预测概率越接近真实概率，该函数值越小，反之越大。

#### 2.11 解释什么是梯度

梯度（gradient）是一个向量，表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大。

#### 2.12 什么是梯度下降

梯度下降是一个最优化算法，常用于机器学习和人工智能当中用来递归性地逼近最小偏差模型，核心思想是**按照梯度相反的方向，不停地调整函数权值**。其步骤为：

1）求损失函数值

2）损失是否足够小？如果不是，计算损失函数的梯度

3）按梯度的反方向走一小步（调整权重，）

4）循环到第2步，迭代执行

#### 2.13 激活函数出现梯度消失会有什么后果

机器学习中，如果模型的优化依赖于梯度下降，梯度消失会导致模型无法进一步进行优化。

#### 2.14 如何解决梯度消失问题

1）更换激活函数：如更滑为relu, leakrelu

2）批量规范化处理-BN(规范化输入，即通过减去其均值并除以其标准差)：通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性

3）使用残差结构：通过引入残差结构，能有效避免梯度消失问题

#### 2.15 什么是梯度爆炸，如何解决梯度爆炸问题

1）梯度爆炸。梯度消失是在计算中出现了梯度过小的值，梯度爆炸则相反，梯度计算出现了过大的值。梯度过大，可能使参数更新幅度过大，超出了合理范围。

2）解决梯度爆炸的方法

- 梯度裁剪：把沿梯度下降方向的步长限制在一个范围之内，计算出来的梯度的步长的范数大于这个阈值的话，就以这个范数为基准做归一化，使这个新的的梯度的范数等于这个阈值
- 权重正则化：通过正则化，可以部分限制梯度爆炸的发生

#### 2.16 什么是批量梯度下降、随机梯度下降，分别有何特点

##### 1）批量梯度下降

① 定义：批量梯度下降（Batch Gradient Descent，BGD）是指在每一次迭代时使用所有样本来进行梯度的更新 ② 特点

- 优点：收敛比较稳定
- 缺点：当样本数目很大时，每迭代一步都需要对所有样本计算，训练过程会很慢

##### 2）随机梯度下降

① 定义：随机梯度下降法（Stochastic Gradient Descent，SGD）每次迭代使用一个样本来对参数进行更新，使得训练速度加快 ② 特点

- 优点：计算量小，每一轮训练更新速度快
- 缺点：收敛不稳定

#### 2.17 什么是学习率，作用是什么

在梯度下降法中，都是给定的统一的学习率，整个优化过程中都以确定的步长进行更新， 在迭代优化的前期中，学习率较大，则前进的步长就会较长，这时便能以较快的速度进行梯度下降，而在迭代优化的后期，逐步减小学习率的值，减小步长，这样将有助于算法的收敛，更容易接近最优解。

#### 2.18 学习率过大或过小会导致什么问题

学习率过大可能导致模型无法收敛，过小导致收敛速度过慢

#### 2.19 什么是反向传播算法，为什么要使用反向传播算法

1）定义

反向传播（Backpropagation algorithm）全称“误差反向传播”，是在深度神经网络中，根据**输出层输出值，来反向调整隐藏层权重的一种方法**

2）**对于多个隐藏层的神经网络，输出层可以直接求出误差来更新参数，但隐藏层的误差是不存在的，因此不能对它直接应用梯度下降，而是先将误差反向传播至隐藏层，然后再应用梯度下降**

#### 2.20 什么是N-gram模型

1）**定义**：N-Gram模型是一种基于统计语言模型，语言模型是一个基于概率的判别模型，它的输入是个句子（由词构成的顺序序列），输出是这句话的概率，即这些单词的联合概率。

N-Gram本身也指一个由N个单词组成的集合，各单词具有先后顺序，且不要求单词之间互不相同。常用的有Bi-gram(N=2)和Tri-gram(N=3)。例如：

句子：L love deep learning

Bi-gram: {I, love}, {love, deep}, {deep, learning}

Tri-gram: {I, love, deep}, {love deep learning}

2）**基本思想**：N-Gram基本思想是将文本里面的内容按照字节进行大小为n的滑动窗口操作，形成了长度是n的字节片段序列。每一个字节片段称为一个gram，对所有gram的出现频度进行统计，并按照事先设置好的频度阈值进行过滤，形成关键gram列表，也就是这个文本向量的特征空间，列表中的每一种gram就是一个特征向量维度。

参考链接：https://baijiahao.baidu.com/s?id=1710297205179446835&wfr=spider&for=pc



#### 2.21 什么是共现矩阵，特点是什么

1）共现（co-occurrence）矩阵指**通过统计一个事先指定大小的窗口内的word共现次数，以word周边的共现词的次数做为当前word的vector**。具体来说，我们通过从大量的语料文本中构建一个共现矩阵来定义word representation。

2）特点

- 优点：矩阵定义的词向量在一定程度上缓解了one-hot向量相似度为0的问题；
- 缺点：但没有解决数据稀疏性和维度灾难的问题。

#### 2.22 Word2vec

##### 1）什么是Word2vec

Word2vec是最近推出的分布式单词表示学习技术，目前被用作许多NLP任务的特征工程技术（例如，机器翻译、聊天机器人和图像标题生成）。从本质上讲，Word2vec通过查看所使用的单词的周围单词（即上下文）来学习单词表示。更具体地说，我们试图通过神经网络根据给定的一些单词来预测上下文单词（反之亦然），这使得神经网络被迫学习良好的词嵌入。

Word2vec通过查看单词上下文并以数字方式表示它，来学习给定单词的含义。所谓“上下文”，指的是在感兴趣的单词的前面和后面的固定数量的单词。假设我们有一个包含N个单词的语料库，在数学上，这可以由以，和表示的一系列单词表示，其中是语料库中的第i个单词。

如果我们想找到一个能够学习单词含义的好算法，那么，在给定一个单词之后，我们的算法应该能够正确地预测上下文单词。这意味着对于任何给定的单词wi，以下概率应该较高：

为了得到等式右边，我们需要假设给定目标单词（wi）的上下文单词彼此独立（例如，wi-2和wi-1是独立的）。虽然不完全正确，但这种近似使得学习问题切合实际，并且在实际中效果良好。

##### 2）Word2vec的优点

- Word2vec方法并不像基于WordNet的方法那样对于人类语言知识具有主观性。
- 与独热编码表示或单词共现矩阵不同，Word2vec表示向量大小与词汇量大小无关。
- Word2vec是一种分布式表示。与表示向量取决于单个元素的激活状态的（例如，独热编码）局部表示不同，分布式表示取决于向量中所有元素的激活状态。这为Word2vec提供了比独热编码表示更强的表达能力。

#### 2.23 optimizer.zero_grad()操作的原因：

因为pytorch里面求每次的偏导都会加上之前的偏导，所以每次更新完都要清掉之前的偏导(梯度)

#### 2.24 为什么要使用激活函数

激活函数（activation function），指神经网络中将输入信号的总和转换为输出信号的函数，激活函数将多层感知机输出转换为非线性，使得神经网络可以**任意逼近任何非线性函数**，这样神经网络就可以应用到众多的非线性模型中。

我们使用激活函数并不是真的激活什么，这只是一个抽象概念，使用激活函数时为**了让中间输出多样化，能够处理更复杂的问题**。

如果不使用激活函数的话，每一层最后输出的都是上一层输入的线性函数，不管加多少层神经网络，我们最后的输出也只是最开始输入数据的线性组合而已。激活函数给神经元引入了非线性因素，当加入多层神经网络时，就可以让神经网络拟合任何线性函数及非线性函数，从而使得神经网络可以适用于更多的非线性问题，而不仅仅是线性问题。
————————————————
原文链接：https://blog.csdn.net/lwc5411117/article/details/83620184

#### 2.25、BLEU计算原理

BLEU全称是：a Method for Automatic Evaluation of Machine Translation(是一种用来评估机器翻译的评价指标)广泛出现在，文本生成的论文当中。
**BLEU采用一种N-Gram的匹配规则，具体来说就是比较译文和参考文献之间的N组词的相似度：**
举个例子：
Source Sentence: 今天天气不错.
Target Sentence: It is a nice day today.
Candidate Sentence：Today is a nice day
当你使用1-Gram匹配时：

![](images\BLEU01.jpg)

可以发现，在你不考虑语序的前提下，Candidate中五个词命中了参考译文，于是Score(1-gram)=5/6
比如，以3-Gram举例子

![](images\BLEU02.jpg)

可以看到Candidate中有两个三元词汇命中了Reference中的四个三元词汇，于是Score(3-gram)=2/4
N-gram匹配法则可以理解为，1-gram匹配时，代表Candidate中有多少个单词是正确翻译的，而2-gram~4-gram匹配就代表了，Candidate的流畅程度

##### BLEU的定义

BLEU(Biligual Evaluation understudy):**是一种基于单词精确度的相似性度量方式，可以用来比较Reference和Candidate中n元组共同出现的程度**

##### **BLEU的优点**

1. 速度快、成本低廉
2. 容易理解
3. 不受语种限制
4. 运用广泛

##### **BLEU的缺点**

1. 忽略同义词
2. N-gram的机制会导致某项分数特别低
3. BLEU不考虑意义
4. BLEU不考虑句子的结构，很多时候，只要单词相同BLEU就会给出很高的分数
5. BLEU不能够很好地处理形态丰富的语言
6. BLEU与人类的判断并不相符合

```python
import math
from nltk.translate.bleu_score import sentence_bleu



reference = [['this', 'is', 'an','test']]
candidate = ['this', 'is', 'a', 'test']
score = sentence_bleu(reference, candidate)#计算真实的BLEU分数

#计算1-gram~4-gram的Pn分数
score_1 = sentence_bleu(reference, candidate, weights=(1,0,0,0))
score_2 = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))
score_3 = sentence_bleu(reference, candidate, weights=(0.33,0.33,0.33,0))
score_4 = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))

score_total = [score_1, score_2,score_3, score_4]
for score in score_total:
    print(score)

Pn_1 = math.log(score_1)
Pn_2 = math.log(score_2)
Pn_3 = math.log(score_3)
Pn_4 = math.log(score_4)

#因为Reference和Candidate的句子长度一致，所以BP惩罚因子为1
BP = 1
W_n = 1/4 #Wn是均匀加权，N的上限取值为4，即最多统计4-gram的精度
BLEU_score= BP * math.exp((Pn_1 + Pn_2 + Pn_3 + Pn_4)/W_n)

print(score)
print(BLEU_score)
```

结合代码，我们可以观察一下BLEU分数的推导分析：

![](images\BLEU03.jpg)

![](images\BLEU04.jpg)

接下来再进行一个实例推导：

![BLEU05](images\BLEU05.jpg)

#### 2.26、BPE原理

- BPE分词算法的由来

- BPE分词算法的流程

- - 词表构建
  - 语料编码
  - 语料解码

##### **BPE分词算法的由来**

BPE算法[1]，其目的是**「使用一些子词来编码数据」**。该方法已经成为了BERT等模型标准的数据预处理处理方式。

在机器翻译领域，模型训练之前一个很重要的步骤就是**「构建词表」**。对于英文语料，一个很自然的想法就是用训练语料中出现过的**「所有英语单词」**来构建词表，但是这样的方法存在两个问题：

- 训练语料中出现过的单词数目很多，这样的构造方式会使得词表变得很大，从而降低训练速度；
- 在模型测试中，很难处理罕见词或者训练过程中没有见过的词（OOV问题）。

另外一种方式是使用单个**「字符」**来构建词表。英文字符的个数是有限的，基于字符的方式可以有效缓解词表数目过大以及OOV的问题，但由于其粒度太细，丢失了很多单词本身所具有的语意信息。

为了解决上述问题，基于Subword（子词）的算法被提出，其中的代表就是BPE算法，**「BPE算法的分词粒度处于单词级别和字符级别之间」**。比如说单词"looked"和"looking"会被划分为"look"，"ed”，"ing"，这样在降低词表大小的同时也能学到词的语义信息。

##### **BPE分词算法的流程**

BPE算法的核心主要分成三个部分：

- 词表构建
- 语料编码
- 语料解码

##### **1、词表构建**

词表构建是BPE算法的核心，其是**「根据训练语料」**来构建BPE算法的词表。算法的整体步骤如下所示：

1. 准备模型的训练语料
2. 确定**「期望的词表大小」**
3. 将训练语料中的所有单词拆分为字符序列，利用这些字符序列构建初始的词表
4. 统计训练语料中每一个连续字节对出现的频率，**「选择出现频率最高的字节对合并成新的subword，并更新词表」**
5. 重复第4步，直到词表大小达到我们设定的期望或者剩下的字节对出现频率最高为1

下面我们通过一个例子来搞懂BPE词表构建的过程。假设我们目前的训练语料中出现过的单词如下，我们构建初始词表：

![img](https://pic3.zhimg.com/80/v2-2e0816bc28d1a2c820368d448474ae76_720w.jpg)

值得注意的是，我们在每一个单词的后面都加入了一个新的字符`<\w>`来表示这个单词的结束。初始的词表大小为7，其为训练语料中所有出现过的字符。

我们之后发现`lo`这个字节对在训练语料中出现频率最高，为3次。我们更新词表，将`lo`作为新的子词加入词表，并删除在当前训练语料中不单独出现的字符`l`和`o`。

![img](https://pic2.zhimg.com/80/v2-bdf051f584c16169d79b0c821f7ea48d_720w.jpg)

之后我们发现`low`这个字节对在训练语料中出现频率最高，为3次。我们继续组合，将`low`加入词表中，并删去`lo`。需要注意的是，由于字符`w`在单词`newer`中仍然存在，因此不予删除。

![img](https://pic2.zhimg.com/80/v2-e4e95848240b8cb19cd9aa305c0acd3d_720w.jpg)

之后我们继续这个循环过程，在词表中加入`er`，并删去字符`r`

![img](https://pic2.zhimg.com/80/v2-285f065a6182c115f6f0552aabb57a49_720w.jpg)

我们一直循环这个过程，直到词表大小达到我们设定的期望或者剩下的字节对出现频率最高为1。

最终我们就得到了基于训练样本构建好的词表。

##### **2、语料编码**

词表构建好后，我们需要给训练语料中的单词进行编码。编码方式如下：

1. 我们首先**「将词表中所有的子词按照长度从大到小进行排序」**
2. 对于每一个给定的单词，我们遍历排序好的词表，寻找词表中的子词是否是该单词的子字符串。如果正好**「匹配」**，则输出当前子词，并对单词剩下的字符串继续匹配
3. 如果遍历完词表，单词中仍然有子字符串没有被匹配，那我们将其替换为一个特殊的子词，比如`<unk>`。

具个例子，假设我们现在构建好的词表为

```text
(“errrr</w>”, 
“tain</w>”, 
“moun”, 
“est</w>”, 
“high”, 
“the</w>”, 
“a</w>”)
```

对于给定的单词`mountain</w>`，其分词结果为：[`moun`, `tain</w>`]

##### **3、语料解码**

语料解码就是将所有的输出子词拼在一起，直到碰到结尾为`<\w>`。举个例子，假设模型输出为：

```text
["moun", "tain</w>", "high", "the</w>"]
```

那么其解码的结果为

```text
["mountain</w>", "highthe</w>"]
```

##### **总结**

在本文中，我们一起学习了BPE的分词算法，该算法是**「利用子词来编码数据」**，已经成为目前机器翻译领域标准的预处理方式。

参考链接：https://zhuanlan.zhihu.com/p/383650769

#### 2.27、机器翻译数据清洗流程：

1、dos2unix

2、繁体转简体(opencc)

3、基础清洗(unicode编码范围正则匹配)

4、champion分句对齐

5、fastalign词对齐

6、normalizer,统一标点符号

7、tokenizer将所有的标点符号转化为&#格式符号

8、jiagu中文分词

9、clean将超过256长度的删掉

10、truecase统一大小写

11、拆分数据集(9.5,0.3,0.2)

12、subword-nmt进行字词编码：learn bpe---apply-bpe

13、二值化

可参考流程：https://blog.csdn.net/qq_42734797/article/details/112916511

### 深度学习基础

1、线性回归

回归：本质就是拟合群体的平均值或者说是群体的趋势y=f(x),回归结果是一个输出值

二分类：结果输出2个值

多分类：输出多个概率值和为1

多分类：

2、softmax回归

主要是公式推导及求导

3、过拟合、欠拟合

概念和作用以及常见预防过拟合和欠拟合措施

4、权重衰退

是为了防治模型过拟合

https://blog.csdn.net/weixin_35949264/article/details/112317832

5、dropout层

是为了防治模型过拟合

6、模型初始化和激活函数

7、Loss

BCE Loss和CE Loss区别：

https://www.jb51.net/article/181634.htm

3、项目陈述部分：模型使用的数据量，大概训练了多长时间，训练的精度mAP是多少，多少个epoch，batch-seize，分类网络的话recall等等指标也都心里应该有数，模型是怎么搭建的，backbone使用的啥结构，有没有使用预训练权重，neck部分使用的啥结构，检测头是啥结构，数据增强怎么做的。

### 深度学习检测tricks

前言：检测技巧考虑的思路：

input----->network----->output	可以从这三部分来考虑

另外，可从训练这一角度来考虑

#### 1、Data Augmentation

- Mixed Up
- Cutout
- CutMix
- Mosaic   # YOLOV4便采用的这种方法进行数据增强

#### 2、Regularization-正则化

- Label Smoothing 标签平滑技术，    # YOLOV4便采用的这种方法

  解释：其实Label Smoothing平滑就是将标签进行一个平滑，原始的标签是0、1，在平滑后变成0.005(如果是二分类)、0.995，**也就是说对分类准确做了一点惩罚，让模型不可以分类的太准确，太准确容易过拟合。**

- DropBlock：对于vector这种可以使用dropout，但对于tensor只能用DropBlock方法，因为卷积有local info share。

#### 3、Activation Function

- Relu
- Swish
- Mish

#### 4、Loss

- L1, L2 & Smooth L1 Loss，一般的网络使用的这个loss
- Iou Loss
- GIou Loss
- DIou Loss
- CIou Loss

### 深度学习算法加速tricks

#### 1、模型设计

选择合适的模型，加快推理速度

减少模型的大小，设计更复杂的模型可以减少欠拟合

#### 2、模型压缩

1. Pruning（修剪）: 因为神经网络很多权重几乎为0，这类参数作用不大，部分参数删掉也不影响模型预测效果

2. Weight Factorization（权重分解）：权重矩阵可以进行低秩矩阵分解，即low-rank matrix
   factorization，从而使得一些参数为0

3. Quantization（削减精度）：能用float32，不用float64；能用int，不用float

4. Weight Sharing（共享权重）：很多layer的参数可以共享，没必要用太多参数

   ------

#### 3、知识蒸馏

> ![](images\加速trick.jpg)
>
> 目标：
>
> a. Increase inference speed 
>
> b. Reduce model size 
>
> 加速方法最重要的：模型设计
>
> 比如MobileNet V1/V2/V3等



### 分类网络总结

#### 1、AlexNet

> 1. CNN卷积神经网络
> 2. **Relu激活函数**
> 3. 双GPU模型训练
> 4. LRN局部响应归一化(Local Response Normalization)
> 5. 重叠最大池化
> 6. 数据增强Data Augmentation
> 7. **Dropout**层
>
> ### AlexNet的特点
>
> - 使用ReLU作为激活函数，并验证其效果在较深的网络超过了Sigmoid，**成功解决了Sigmoid在网络较深时的梯度消失问题**
> - 使用Dropout（丢弃学习）随机忽略一部分神经元防止过拟合
> - 在CNN中使用重叠的最大池化。此前CNN中普遍使用平均池化，AlexNet全部使用最大池化，**避免平均池化的模糊化效果**
> - 提出了LRN（Local Response Normalization，局部正规化）层，对局部神经元的活动创建竞争机制，使得其中响应比较大的值变得相对更大，并抑制其他反馈较小的神经元，**增强了模型的泛化能力**
> - 使用CUDA加速深度卷积网络的训练，利用GPU强大的并行计算能力，处理神经网络训练时大量的矩阵运算
>
> ![img](https://pic4.zhimg.com/80/v2-04f8b8a058d88d1e804a2a021324593b_720w.jpg)
>
> ![](D:\Desktop\8期CV课\项目一(车道线分割)\images\Screenshots\Screenshot_20210822_171736_tv.danmaku.bili.jpg)
>
> 使用了局部正则化可以防治过拟合，具体可以参考上图.。
>
> LRN这个操作是在Relu激活之后进行操作的LRN
>
> 实验证明这个操作能降低错误率，提高准确率。为什么呢？？？？？？？？？？？？？？
>
> dropout层就是让它的输出为0(50%的概率)，防止过拟合
>
> ​	多个模型集成可以有效防治过拟合，但对于大型神经网络来说不经济,训练阶段，每一个batch随机掐死一半的神经元(将神经元输出为0)，阻断该神经元的前向-反向传播，预测阶段保留所有神经元，预测结果乘以0.5
>
> dropout层为什么能减少过拟合：----具体可以参考下面插图论文所述部分
>
> 1、模型集成，p=0意味着2的n次方个共享权重额潜在网络；
>
> 2、记忆随机抹去，不再死记硬背；
>
> 3、dropout层减少神经元之间的联合依赖性，每个神经元都被逼着独当一面；
>
> 4、有性繁殖，每个基因片段都要与来自另一个随机个体的基因片段协同合作；
>
> 5、数据增强，总可以找到一个图片，使神经网络中间层结果与dropout层相同，相当于增加了这些图片到数据集中；
>
> 6、稀疏性。
>
> 7、共享权重
>
> 由论文原文知：
>
> 防止过拟合原文采用了两种方法：
>
> 1. data augmentation
> 2. dropout
>
> AlexNet网络的pytorch实现：
>
> ```python
> import time
> import torch
> from torch import nn, optim
> import torchvision
> 
> 
> class AlexNet(nn.Module):
>     def __init__(self):
>         super(AlexNet, self).__init__()
>         self.conv = nn.Sequential(
>             nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding
>             nn.ReLU(),
>             nn.MaxPool2d(3, 2), # kernel_size, stride
>             # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
>             nn.Conv2d(96, 256, 5, 1, 2),
>             nn.ReLU(),
>             nn.MaxPool2d(3, 2),
>             # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。
>             # 前两个卷积层后不使用池化层来减小输入的高和宽
>             nn.Conv2d(256, 384, 3, 1, 1),
>             nn.ReLU(),
>             nn.Conv2d(384, 384, 3, 1, 1),
>             nn.ReLU(),
>             nn.Conv2d(384, 256, 3, 1, 1),
>             nn.ReLU(),
>             nn.MaxPool2d(3, 2)
>         )
> 
>          # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合
>         self.fc = nn.Sequential(
>             nn.Linear(256*5*5, 4096),
>             nn.ReLU(),
>             nn.Dropout(0.5),
>             nn.Linear(4096, 4096),
>             nn.ReLU(),
>             nn.Dropout(0.5),
>             # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000
>             nn.Linear(4096, 10),
>         )
> 
>     def forward(self, img):
>         feature = self.conv(img)
>         output = self.fc(feature.view(img.shape[0], -1))
>         return output
> ```
>
>

#### 2、VGG

> **VGG16网络结构：**
>
> 13个卷积层(3x3的卷积核，2层->2层->3层->3层->3层)+5个max pooing池化层(2^5倍的降维)+3个FC层+softmax
>
> **网络特点：**
>
> **网络深，卷积核小，池化核小**（与AlexNet的3x3池化核相比，VGG全部用的是2x2的池化层）**全连接转卷积**。（网络测试阶段将训练阶段的三个全连接替换为三个卷积（1个conv7x7，2个conv1x1），测试重用训练时的参数，使得测试得到的全卷积网络因为没有全连接的限制，因而可以接收任意宽和高的输入。）
>
> **优点：**
>
> - 1、卷积核的大小影响到了参数量，感受野，VGG用较深的网络结构和较小的卷积核**既可以保证感受视野，又能够减少卷积层的参数**，比如**两个3x3的卷积层叠加**等价于一个5x5卷积核的效果，3个3x3卷积核叠加相加相当于一个7x7的卷积核，而且**参数更少**。大约是7x7卷积层的（3x3x3）/（7x7）=0.55，三个卷积层的叠加，对**特征学习能力更强**;
>
> - 2、池化层：从AlexNet的kernel size为3x3，stride为2的max-pooling改变为kernel size均为2x2，stride为2的max-pooling，**小的池化核能够带来更细节的信息捕获**（当时也有average pooling，但是在图像任务上max-pooling的效果更好，max更加容易捕捉图像上的变化，带来更大的局部信息差异性，更好的描述边缘纹理等，用average-pooling可能会使得图像模糊了，类似与数字图像处理的高斯模糊）
>
> - 其他特点：
>
>   全连接层：特征图的高度从512后开始进入到全连接层，也就是说全连接层前是7x7x512维度的特征图（大概为25000），这个全连接层过程要将25000映射到4096,（大约将原来的信息压缩成原来的五分之一），有可能是这个压缩过程太急于是又接了一个fc4096作为缓冲。使用1x1卷积核：选用卷积核的最直接原因是在维度上继承全连接，在这里可以增加决策函数的非线性能力
>
> - **1x1卷积的特点**：
>
>   - 不考虑单通道上像素的局部信息，专注于一个卷积核内部通道的信息整合；
>   - 对feature map**降维或者升维**，例如224x224x100的图像（或feature map）经过20个conv1x1的卷积核，得到224x224x20的feature map。尤其当卷积核（即filter）数量达到上百个时，3x3或5x5卷积的计算会非常耗时，所以1x1卷积在3x3或5x5卷积计算前先降低feature map的维度。
>
> ------
>
>
>
> ![](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_230843_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_230936_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_230936_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_231010_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_231010_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_231344_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_231344_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_231425_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_231425_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_231812_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_231812_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_231833_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_231833_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_231853_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_231853_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_232342_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_232342_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_232548_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_232548_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_232724_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_232724_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_232737_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_232737_tv.danmaku.bili.jpg)
>
> ![Screenshot_20210808_232946_tv.danmaku.bili](D:\Desktop\8期CV课\电子书论文\子豪兄论文合集\VGG\Screenshot_20210808_232946_tv.danmaku.bili.jpg)

> VGG网络的pytorch实现：
>
> ```python
> # 一个 vgg 的 block，传入三个参数，第一个是模型层数，第二个是输入的通道数，第三个是输出的通道数，第一
> #层卷积接受的输入通道就是图片输入的通道数，然后输出最后的输出通道数，后面的卷积接受的通道数就是最后
> #的输出通道数
> def vgg_block(num_convs, in_channels, out_channels):
>     net = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), nn.ReLU(True)] # 定义第一层
>     
>     for i in range(num_convs-1): # 定义后面的很多层
>         net.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))
>         net.append(nn.ReLU(True))
>         
>     net.append(nn.MaxPool2d(2, 2)) # 定义池化层
>     return nn.Sequential(*net)
> 
> #函数对 vgg block 进行堆叠
> def vgg_stack(num_convs, channels):
>     net = []
>     for n, c in zip(num_convs, channels):
>         in_c = c[0]
>         out_c = c[1]
>         net.append(vgg_block(n, in_c, out_c))
>     return nn.Sequential(*net)
> 
> #vgg模型
> class vgg(nn.Module):
>     def __init__(self):
>         super(vgg, self).__init__()
>         self.feature = vgg_net
>         self.fc = nn.Sequential(
>             nn.Linear(512, 100),
>             nn.ReLU(True),
>             nn.Linear(100, 10)
>         )
>     def forward(self, x):
>         x = self.feature(x)
>         x = x.view(x.shape[0], -1)
>         x = self.fc(x)
>         return x
> ```
>
>

#### 3、MobileNetV1

前言：Depthwise(DW)卷积与Pointwise(PW)卷积，合起来被称作Depthwise Separable Convolution

> MobileNetV1创新点：
>
> - Depthwise Separable Convolution(大大减少运算量和参数数量) 
>
> 备注：一个卷积核的参数=K x K x K_channel x K_num+bias
>
> ​	   一个卷积层的参数=K x K x (K_channel) x (K_num) x (O_channel)+bias
>
> - 增加超参数α、β
>
> ![](images\Depthwise Convolution.jpg)
>
> ![Pointwise Convolution](images\Pointwise Convolution.jpg)

#### 4、MobileNetV2

> 1、MobileNet V2的创新点
>
> - Inverted Residuals（倒残差结构） 
> - Linear Bottlenecks
> - 使用了ReLU6激活函数
>
> ![](images\Inverted Residuals.jpg)
>
> ![](images\BottleNeck.jpg)
>
> ![](images\Relu6.jpg)

#### 5、MobileNetV3

MobileNetV3网络创新点

> **1.加入SE模块**
>
> **2.更新了激活函数,使用h-swish激活函数**
>
> swish的作者认为，该函数具有无上界、有下界、平滑、非单调的特点，在深层模型上优于ReLU。但是，由于sigmoid函数计算复杂（sigmoid(x) = (1 + exp(-x))^(-1)），所以V3改用近似函数来逼近swish，这使其变得更硬（hard）。
>
> 作者选择了ReLU6作为这个近似函数，有两个原因：
>
> 1、在几乎所有的软件和硬件框架上都可以使用ReLU6的优化实现；
>
> 2、ReLU6能在特定模式下消除由于近似sigmoid的不同实现而带来的潜在的数值精度损失
>
> 作者认为，随着网络的深入，应用非线性激活函数的成本会降低，能够更好的减少参数量。作者发现swish的大多数好处都是通过在更深的层中使用它们实现的。因此，在V3的架构中，只在模型的第一层和后半部分使用h-swish(HS)。
>
>
>
> ![](images\MobileNet-V3激活函数.jpg)

### 目标检测网络总结

#### 1、RCNN

> 简介：RCNN网络是深度学习目标检测的开山之作，以**AlexNet**网络作为backbone部分。
>
> ##### RCNN网络结构
>
> ![](2、项目介绍及One-Stage检测算法\week1-4 Detection-3 stages_2021\week1-4 Detection-3 stages_2021_19.jpg)
>
> ##### RCNN网络算法基本流程
>
> 1)、1张图像生成2000个候选框
>
> 2)、对每一个候选框，通过CNN提取特征
>
> 3)、将特征送入每一类的SVM分类器，判断是否属于该类
>
> 4)、使用回归器精细修正候选框位置
>
> ##### 流程详解
>
> 1)、候选区域的生成
>
> 利用SS算法通过图像分割的方法得到一些原始区域，然后使用一些合并策略将这些区域合并，得到一个层次化的区域结构，而这些结构就包含着可能需要的物体。
>
> ![](D:\01_workspace\deep-learning-for-image-processing\course_ppt\目标检测网络相关课件\R-CNN\R-CNN_04.jpg)
>
> 2)、对每个候选区域，使用深度网络提取特征
>
> 将2000候选区域缩放到227x227大小，接着将候选区域输入事先训练好的AlexNet-CNN网络获取4096维的特征得到2000x4096维矩阵。
>
> ![](D:\01_workspace\deep-learning-for-image-processing\course_ppt\目标检测网络相关课件\R-CNN\R-CNN_05.jpg)
>
> 3)、特征送入每一类的SVM分类器，判定类别
>
> 将2000x4096维特征与20个SVM组成的权值矩阵4096x20相乘，获得2000x20维矩阵表示每个建议框是某个目标类别的得分。分别对上述2000x20维矩阵中每一列即每一类进行非极大值抑制剔除重叠建议框，得到该列即该类中得分最高的一些建议框。
>
> ![](D:\01_workspace\deep-learning-for-image-processing\course_ppt\目标检测网络相关课件\R-CNN\R-CNN_06.jpg)
>
> ![](D:\01_workspace\deep-learning-for-image-processing\course_ppt\目标检测网络相关课件\R-CNN\R-CNN_07.jpg)
>
> ![](D:\01_workspace\deep-learning-for-image-processing\course_ppt\目标检测网络相关课件\R-CNN\R-CNN_08.jpg)
>
> 4)、使用回归器精细修正候选框位置
>
> 对NMS处理后剩余的建议框进一步筛选。接着分别用20个回归器对上述20个类别中剩余的建议框进行回归操作，最终得到每个类别的修正后的得分最高的bounding box。
>
> 如图，黄色框P表示建议框Region Proposal,绿色框G表示实际框Ground Truth,红色框G撇表示Region Proposal进行回归后的预测框，可以用最小二乘法解决的线性回归问题。
>
> ![](D:\01_workspace\deep-learning-for-image-processing\course_ppt\目标检测网络相关课件\R-CNN\R-CNN_09.jpg)
>
> #### RCNN的缺点
>
> 1. slow：need to run full forward pass if CNN for each region proposal
>
> 2. SVMs and regressors are post-hoc:CNN features not updated in response to SVMs and regressors.
>
> 3. Complex multistage training pipeline
>
> 4. 测试速度慢：
>
>    测试一张图片约3s(CPU)。用SS算法提取候选框用时约2秒，一张图像内候选框之间存在大量重叠，提取特征操作冗余。
>
> 5. 训练速度慢：过程及其繁琐
>
> 6. 训练所需空间大：
>
>    对于SVM和bbox回归训练，需要从每个图像中的每个目标候选框提取特征，并写入磁盘。对于非常深的网络，如VGG16，从VOC07训练集上的5k图像上提取的特征需要数百GB的存储空间。

#### 2、Fast RCNN

> 简介：
>
> Fast RCNN同样使用**VGG16**作为网络的backbone部分，与RCNN相比训练时间快9倍，测试推理时间快213倍，准确率从62%提升至66%(VOC数据集上)。
>
> ##### Fast RCNN网络结构
>
> ![](images\Fast RCNN.jpg)
>
> ##### Fast RCNN网络的基本流程
>
> 1)、1张图像生成2000个候选区域(通过SS算法)
>
> 2)、将图像输入CNN网络得到相应的特征图，将SS算法生成的候选框投影到特征图上获得相应的特征矩阵
>
> 3)、将每个特征矩阵通过ROI pooling层缩放到7x7大小的特征图，接着将特征图展平通过一系列FC层得到预测结果
>
> ##### 优点：
>
> 1)、一次性计算整张图像特征
>
> 将整张图像送入CNN网络，紧接着从特征图像上提取相应的候选区域。这些候选区域的特征不需要再重复计算。
>
> 2)、
>
> ##### RIO Pooling Layer
>
> 1)、目的就是将不同size的region proposal变成相同的size然后送进CNN网络
>
> 但是普通的ROI Pooling操作存在精度损失，因为像素点都是离散数据，通过打格的方法肯定不一定都能落在合适的像素点组成的格子上。
>
> 所以就有了后面的ROI Align方法和Precision ROI方法。
>
> ##### 分类器
>
>
>
> ```python
> # 听课笔记：
> 1、ROI Pooling的目的：
> 将不同size的ROI变成相同的size
> 2、相比于RCNN,多了ROI Projection
> ######################################3、FAST RCNN流程##########################
> B0.Region Proposal:same as RCNN
>    2K per image.record location for each ROI
> B1&B2.Convolution & Projection
> 	a.do conv for image,project location for each ROI
>     b.3 basic structures provided,use Vgg16 as an E.g.
>     c.4 max pooling,/16 # 就是4层，除以16
> B3. ROI Pooling(是在feature map当中处理)
> 	a.grid each ROI in feature map to fixed size,and do max pooling within each grid
>     b. so different size of feature maps can transfer into feature maps with same size
> # 代码中表示物体：
> 			  左上角和右上角(x1,y1,x2,y2)
> 			  中心点坐标和框的宽高(x,y,w,h)
> # VIP:
>     region proposal是在原图当中，而ROI projection是在对的feature map当中，是有一一对应关系的        
> B4. FC Layers
> 	fc layer cost a lot,can use SVD to accerate（这句话就是扯淡，原文作者在代码里面就根本没有用这个加速方法，面试可不提这一点）
> B5. Multi-task Loss
> ```
>
> 1、ROI Pooling的目的：
> **将不同size的ROI变成相同的size**
> 2、相比于RCNN,多了ROI Projection
> 3、FAST RCNN流程
> B0.  Region Proposal:same as RCNN
>    2K per image.record location for each ROI
> B1&B2.   Convolution & Projection
> ​	a.do conv for image,project location for each ROI
> ​        b.3 basic structures provided,use VGG16 as an E.g.
> ​        c.4 max pooling,/16  # 就是4层，除以16
> B3.   ROI Pooling(是在feature map当中处理)
> ​	a.grid each ROI in feature map to fixed size,and do max pooling within each grid
> ​        b. so different size of feature maps can transfer into feature maps with same size
>
> ##### 代码中表示物体：
>
> 左上角和右上角(x1,y1,x2,y2)
>  中心点坐标和框的宽高(x,y,w,h)
>
> ##### VIP:
>
> ```
> region proposal是在原图当中，而ROI projection是在对的feature map当中，是有一一对应关系的        
> 
> ```
>
> B4. FC Layers
> ​	fc layer cost a lot,can use SVD to accerate（这句话就是扯淡，原文作者在代码里面就根本没有用这个加速方法，面试可不提这一点）
> B5. Multi-task Loss
>
>

#### 3、Faster RCNN

前言：首次提出RIO操作

> Faster RCNN存在的问题：
>
> - 对小目标检测效果不好，因为最后网络深层输出只有一层feature map，当然会损失网络的一些细节信息，因为特征矩阵越大，保留图像的细节信息更多一点，相反细节信息会更少。
> - 模型大，检测速度慢，因为是两阶段目标检测，有2处loss
>
> ![](2、项目介绍及One-Stage检测算法\week1-4 Detection-3 stages_2021\week1-4 Detection-3 stages_2021_42.jpg)

> ```python
> # 听课笔记
> Faster R-CNN[2015]:
> 结构如上图所示：整体结构有三块，
> C1.Backbone
> 	a.Aim:extract feature integratedly
>         -No need to extract feature per ROI;
>         -Use generated feature to generate proposal
>     b.structure:ZF/ResNet/VGG16(13conv+13ReLU+4Pooling)
>     c.output:B x C x H/16 x W/16
>         	 1 x 256 x 38 x 50
>              [feature map]  # 这里backbone输出的就是feature map。
> 网络结构如上所示：
> C2.RPN
> 网络上面部分是classification，下面一部分是regression，1x1操作是为了改变channel的个数，上面channel变成了18，下面regression变成了36.
> @1.Aim:generate region proposal[real detected objects are coming from RPs]
>  	%1.it's the reason where the name "two -stage" coming from:RPN+Bbox Regression
>     %2.it's the reason why some argue two-stage detection has better results then one-stage methods
> @2.output of RPN:
>     %1.ROIs:128x5
>         [0,x1,y1,x2,y2]->physical region proposal; # 备注：0就是占位
>     %2.Label:128
>         [0~20]-> RIOs' classification   # 备注：0对应的是background
>     %3.bbx_target:128x84
>         [(20+1)x4]->targets for bounding box regression # 4代表的是x1,y1,x2,y2四个参数
> 	%4.bbx_weight:128x84
>         [0 or 1]->weights for bx_target when box regressing # 0就是我们的背景，1就是我们的前景。
>    train和test最大的差距就是train多了回归loss和分类loss
> C2.1:Classification branch
>     18=9x2,2就是0/1,9个anchor
>     anchor是如何产生的？
>     答案：原始图像经过CNN网络得到很多Tensor-feature map，feature map中的每一个点point对应原始图像中的一个块(代表原始图像的一个区域)，原始的anchor我们选择16*16（2:1,1:1；1:2），分别会乘以8/16/32:  128*128,256*256,512*512，所以基本会覆盖掉大中小三种size的物体。每一个特征点代表着9个anchor。
>     # structure：anchor****************************************************
>     %1.represent an area in original image
>     %2.have different scale & ratio to cover all kinds of objects
>     %3. 9 anchors in total:3 scales x 3 ratios
> 	%4. 1x9x38x50=17100 anchors        
>     %5. coordinates of anchors are of original images
> 备注：每一个feature map中的pixel点有9个anchor，每个anchor对应原图中的一个小区域
> C2.1.1:classification reshape
>     因为softmax结果是二分类，所以要把feature map搞成2个channel，所以38x9x2(将18拆成9x2，然后摞起来。)
> C2.1.2:softmax
>     2 branches:
>         a.bp loss
>         b.no loss/get score
> 	@1、1 foreground & 0 background.
>     	get loss with anchor and pass back.
>     	use IOU:iou<0.3标注为0，iou>0.7标注为1，如果是在0.3~0.7在代码中我们就ignore掉。
>     @2、just do classification and get the score,
>     	use the score to select proposal.
> # anchor的压缩：
> 	第一次压缩：超出图像边界的anchor我们ignore了        
>     第二次压缩：ignore
>     第三次压缩：sofmax之后对iou进行一个排序，完了取前12000个
>     第四次压缩：NMS
> C2.1.3: cls reshape
>     @1.back to original shape:reshape成38x50
>     @2.each voxel(像素) represents the possibility of being a foreground or a background of an anchor.
> C2.2:regression
>     channel是36：每一个pixel有9个anchor，每一个anchor有4个坐标
>     1x36(9x4,4个坐标(x0,y0,x1,y1))x38x50
> 
> # 回归问题
> C2.2.1: Smooth L1 Loss
>     @1. need to multiply the mask
>     	%1.anchor=1,mask=1
>         %2.anchor=0,-1,mask=0
>     @2. smooth L1 Loss：L1 Loss在0处不可导,L2 Loss处处连续可导（图像与y=x^2类似）
>     公式(代码中有定义好的接口)见课件：
> # Regress（center）offset,Not coordinates
>   这种方法是现在好多地方都在用的方法，所以必须得掌握
> 公式里使用log的原因：suppress greater bbox，能直接回归。
> # 为啥不能直接回归坐标而是要用offset回归？
>     因为直接回归对小物体是致命打击**************************************
>     @)、why use exp
>         tx & ty is used for scaling we need to ensure scale>0
>     @)、why use log
>         a、reversing procedure of exp
>         b、suppress greater bbox
> C2.3:get proposal
>     @1.regard anchor as FG when IOU>0.7
>     @2.regard anchor as BG when IOU<0.3
>     @3.regardless other anchors
>     @4.then just keep 128:0.25fg+0.75bg anchors
> C3. After RPN
> 	same as Fast RCNN
> C4. how to train
> 	@1.Classic:alternatively 4 steps
>         %1. use imageNet finetune(微调) our RPN
>         %2. use trained proposal from step1(RPN的输出) to train an Fast RCNN
>         %3. use detected results to initialize RPN training where we freeze the backbone layers but to train pure RPN-related layers
>         %4.finetune fast RCNN-related layers only.
>     @2.other methods:
>         we can also train Faster RCNN as a whole in just one step
> # Faster RCNN用的不是非常多，主要用的是它提出的anchor，实际场景当中运用的还是很少。
> 下图右边图多了俩loss
> ```
>
> **RPN网络结构详解**
>
> ![](images\RPN000.jpg)
>
> ![RPN00](images\RPN00.jpg)
>
> ![RPN01](images\RPN01.jpg)
>
> ![RPN02](images\RPN02.jpg)
>
> ![RPN03](images\RPN03.jpg)
>
> ![RPN04](images\RPN04.jpg)
>
> ![RPN05](images\RPN05.jpg)
>
> ![RPN06](images\RPN06.jpg)
>
> ![RPN如何映射到FPN哪个特征层](images\RPN如何映射到FPN哪个特征层.jpg)

#### 4、SSD

前言：2016年发表的论文，对于输入尺寸300x300的网络，在VOC2007数据集上可达到74.3%的mAP和59FPS，达到真正的实时检测算法，对于512x512的输入，达到了76.9%mAP超越当时最强的Faster RCNN(73.2%)。

备注：SSD属于onestage检测算法。

Faster RCNN存在的问题：

- 对小目标检测效果很差：只是在1个特征层上预测，而且都是经过很多层抽象过的特征层
- 模型大，检测速度较慢：存在2次检测，在RPN部分有1次检测，在最后Fast RCNN部分有1次检测。

> 网络结构如下图：
>
> ![](images\SSD.jpg)
>
> 注意网络实现细节：
>
> s2代表stride=2，padding=1，s1代表stride=1，padding=0
>
> ------
>
> backbone部分采用VGG16的结构
>
> 在不同特征尺度上预测不同尺度的目标：feature map大的来检测小目标，相反，feature map小的检测大目标，就是特征层越往后面越抽象feature map也越小。
>
> SSD有6个预测特征层，每个预测头都是用**3x3的卷积核**进行来预测的
>
> 结构的好处：浅层网络的feature map去预测小目标，深层网络去预测大目标

#### 5、YOLO系列

##### YOLOV1-2016

> ![](images\V1.jpg)
>
> Final output tensor: S x S x (5 * B + C)  -----[7 x 7 x (5 * 2 + 20) ] -> v1
>
> **优点**：one-stage，really fast 
>
> **缺点**：
>
> - bad for crowed objects
> - bad for small objects
> - bad for objects with new width-height ratio
> - no BN
>
> **YOLOV1的优点**：
>
> 端到端的网络，流程简化了，检测速度快相比RCNN系列，但是mAP比较低。
>
> **YOLOV1的缺点**：
>
> 1、定位能力比较差，因为是7x7的grid cell，所以只能检测出49个物体，所以对小目标性能就比较差，密集目标能力也差
>
> 2、YOLOV1里面最后有FC层，所以输入的图片大小必须是固定的

##### YOLOV2-2017

前言：**首次提出anchor的概念**

> ![V2](images\V2.jpg)
>
> 1、**加入了BN层**-----------increase of 4% mAP,实际操作就是将神经元的输出-均值除以方差，集中到以0为中心的附近区域
>
> 作用：将神经元的输出通过BN操作都集中到0附近，这样都在激活函数曲线的非饱和区域，防止过拟合，加快梯度的收敛，可以取代Dropout层
>
> 思考：BN层与Dropout为什么不能一起使用？
>
> 答案：
>
> 2、**High Resolution Classifier**
>
> Resize & Finetune on ImageNet (448 x448)，为了让模型适应并能够预测高分辨率的图片
>
> 3、**加入了 anchor**:   通过对GT聚类得到的(作者事先聚类得到设计好的尺寸)
>
> YOLOV2里面anchor的数量是5个，每个grid cell对应5个anchor
>
> 模型输出的feature map大小是奇数是有原因的:anchor中心点可以落在grid cell网格上
>
> 4、**Fine-Grained Features**：底层的细粒度特征和高层的语义层特征进行了融合
>
> - Lower features are concatenated directly to higher features
> - A new layer is added for that purpose: reorg或者PassThrough层
>
> 5、**Multi-Scale Training**
>
> - **Remove FC layers**: Can accept any size of inputs, enhance model robustness. 
> - Size across 320, 352, …, 608. Change per 10 epochs 
>
>   [border % 32 = 0, decided by down sampling]
>
> 对于YOLOV2来说，因为最后将FC层替换成了Global average pooling,所以对输入图片尺度没有啥限制，可以进行多尺度训练

##### YOLOV3-2018

> 参考链接：https://blog.csdn.net/weixin_44791964/article/details/105310627?spm=1001.2014.3001.5501
>
> **网络结构：**
>
> 网络结构backbone部分采用Dark-Net53作为骨干网络，结构中有使用残差模块，也有2次上采样，输出有3个输出检测头，1个是13x13的feature map预测大目标，1个是26x26的feature map来预测中型目标,1个是52x52的feature map来预测小型目标。
>
> ![V3](images\V3.jpg)
>
> YOLOV3改进点：
>
> - New structure：将残差结构进行了改造
>
> - Multiscale Structure：3 scales; 3 anchors per scale per grid
>
> - Change Classification：80 classes, from softmax —> logistic
>
> - **FPN-Net**
>
>   > **Pros:** 
>   >
>   > 1. Lower layers have accurate localization info; 底层网络含有比较精确的位置信息
>   >
>   >    Higher layers have ample semantic info ；深层网络含有比较好的语义信息
>   >
>   >    FPN combine them together. Like U-Net ；FPN结构将这二者很好地结合到了一起。
>   >
>   > 2. Feature detector: worked as a part in a whole bigger network. 
>
>   > **Details:** 
>   >
>   > 1. 3x3convolution is appended on each merged map to  
>   >
>   >    generate the final feature map to reduce the aliasing effect 
>   >
>   > 2. Feature dimension at output is 256. 
>   >
>   > 3. When used in FPN. P2-P6. Anchor areas: 322, ~, 5122 per scale, 
>   >
>   >    with ratio {1:2,1:1,2:1}, 15 in total. And, predictor head is shared. 
>
>   [Bubbliiiing博主]V3听课笔记：
>
>   第一步：(x,y,w,h)是先验框anchor相对于原图的坐标尺寸；
>
>   第二步：(x,y,w,h)/stride得到相对于feature map的坐标尺寸，其中stride=416/13=32,stride=416/26=16，stride=416/52=8；
>
>   第三步：
>
>   先验框(anchor)的中心点就是13x13的**角点**或者说是网格点，就是它经过sigmod函数之后变为0~1之间，这样它只能调整到**右下角正方形网格**里面，注意坐标轴是向下为y轴，向右为x轴，啊终于大彻大悟了！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！
>
>   x,y要经过sigmod函数归一化到0~1之间，但是w,h就不需要了，因为你把它调整到右下角方格里面宽高是不变的。哈哈哈哈哈我终于理解了！！！！！！！！！！！！！！！！！！
>
>   先验框调整过程：其实就是将方格点的坐标调整到右下角**方格内**的过程
>
>   先调整x,y坐标的过程就是做平移操作--》对应公式上面两行
>
>   然后调整w,h的过程就是做缩放的操作--》对应公式下面两行
>
>   ![](images\先验框调整过程.jpg)
>
>   NMS操作的时候：就是将x,y,w,h中心点坐标+宽高的形式转化成左上角坐标+右下角坐标的形式。
>
>   对所有的类进行循环，然后进行通过置信度排序从高到低，最高的框分别与剩下的框进行一一IOU操作，大于IOU阈值的可能和最高的框预测的是同一个物体然后筛选掉，小于IOU 阈值的保留下面。
>
>   训练的时候可与冻结部分权重，防治过拟合。
>
>   先是p->a,完了是g->p
>
>   看下面这张图你会顿悟：
>
>   **anchor**作为中间变量
>
>   ![](images\YOLO-V2-V3回归原理.jpg)

##### YOLOV4-2020

> ![](images\YOLOV4.png)
>
> **YOLOV4改进部分**（不完全）
> 1、主干特征提取网络：DarkNet53 => CSPDarkNet53
>
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200509113651540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70)
>
> 2、**特征金字塔：SPP（空间金字塔池化），PAN**
>
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200509215346310.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70)
>
> SPP作用：**它能够极大地增加感受野，分离出最显著的上下文特征**
>
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200509214653559.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70)
>
> 上图为原始的PANet的结构，可以看出来其具有**一个非常重要的特点就是特征的反复提取**。
> 在（a）里面是传统的特征金字塔结构，在完成特征金字塔从**下到上的特征提取后**，还需要**实现（b）中从上到下的特征提取。**
>
> 而在YOLOV4当中，其主要是在**三个有效特征层上使用了PANet结构。**
>
> 3、分类回归层：YOLOv3（未改变）
>
> 4、训练用到的小技巧：Mosaic数据增强、Label Smoothing平滑、CIOU、学习率余弦退火衰减
>
> - **Label Smoothing**平滑
>
>   其实Label Smoothing平滑就是将标签进行一个平滑，原始的标签是0、1，在平滑后变成0.005(如果是二分类)、0.995，**也就是说对分类准确做了一点惩罚，让模型不可以分类的太准确，太准确容易过拟合**。
>
> - **CIOU**：	
>
> ​	IoU是比值的概念，对目标物体的scale是不敏感的。然而常用的BBox的回归损失优化和IoU优化不是完全等价的，寻常的IoU无法直接优化没有重叠的部分。
>
> 于是有人提出直接使用IOU作为回归优化loss，CIOU是其中非常优秀的一种想法。
>
> **CIOU将目标与anchor之间的距离，重叠率、尺度以及惩罚项都考虑进去，使得目标框回归变得更加稳定**，不会像IoU和GIoU一样出现训练过程中发散等问题。而惩罚因子把预测框长宽比拟合目标框的长宽比考虑进去。
>
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200425144646161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70)
>
> CIOU公式如下：
>
> ![](images\CIOU.jpg)
>
> ![CIOU01](images\CIOU01.jpg)
>
> - **余弦退火衰减法**
>
> ​	**学习率会先上升再下降**，这是退火优化法的思想。**上升的时候使用线性上升，下降的时候模拟cos函数下降**。执行多次。
>
> ![在这里插入图片描述](https://img-blog.csdnimg.cn/20200519133028199.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDc5MTk2NA==,size_16,color_FFFFFF,t_70)
>
> 5、**激活函数：使用Mish激活函数**
>
> 结果相比sigmod和tanh激活函数要**更加平滑**
>
> 从图中可以看出他在负值的时候并不是完全截断，而是允许比较小的负梯度流入，从而保证信息流动
>
> 并且激活函数无边界这个特点，让他**避免了饱和**这一问题，比如sigmoid，tanh激活函数通常存在梯度饱和问题，在两边极限情况下，梯度趋近于1，而Mish激活函数则巧妙的避开了这一点，另外**Mish函数也保证了每一点的平滑**，从而使得梯度下降效果比Relu要好。
>
> ![](images\Mish.jpg)
>
> 原文链接：https://blog.csdn.net/weixin_44791964/article/details/106014717
>
> ------
>
>
>
> ##### **Anchor Free Methods**
>
> Anchor free net is a trend 
>
> • Lots of hyperparameters: sizes, aspect-ratios, number of anchors, …… 
>
> • Hard to generalize: different datasets have different data shape, need  to redesign 
>
> • Difficult to train: unbalanced positive / negative samples 
>
> • Complex calculation 
>
> • Myriads of redundancy
>
> 应用：CornerNet、**CenterNet**、FCOS等都使用了anchor free 机制。
>
> ##### **Focal loss**
>
> Q. Why one-stage performs worse than two stage? 
>
> ​	Because neg/pos samples are extremely unbalanced ;
>
> ​	Gradient is dominated by easy samples. 
>
> S. We can use FOCAL LOSS to solve it.

##### YOLOV5-2020

> 网络结构：
>
> ![preview](https://pic1.zhimg.com/v2-15e53f82f68e62ce1ea9a565121e21f8_r.jpg)
>
> 参考链接：https://zhuanlan.zhihu.com/p/172121380

#### 6、FCOS一阶全卷积目标检测

前言：

> FCOS:一阶全卷积目标检测，Fully Convolutional One-Stage Object Detection
>
> 该算法是一种基于FCN的逐像素目标检测算法，实现了无锚点（anchor-free）、无提议（proposal free）的解决方案，并且提出了中心度（Center—ness）的思想，同时在召回率等方面表现接近甚至超过目前很多先进主流的基于锚框目标检测算法
>
> 参考链接：https://zhuanlan.zhihu.com/p/63868458



>

### 图像分割网络总结

#### 1、**ResNet**

> 背景：CNN训练存在的问题：
>
> 1、梯度消失与梯度爆炸：因为很深的网络选择了不合适的激活函数，在很深的网络中进行梯度反传，梯度在链式法则中就会变成0或者无穷大，导致系统不能收敛，然后梯度弥散/爆炸在很大程度上被合适的激活函数Relu、牛逼的网络初始化kaiming初始化、BN层等trick给处理了。
>
> 2、梯度弥散：当深度开始增加的时候，accuracy经常会达到饱和，然后开始下降，但并不是由于过拟合引起的。
>
> ResNet本身是一种拟合残差的结果，让网络学习任务更简单，可以**有效地解决梯度弥散问题**。
>
> **ResNet的优点：**
>
> - 引入**跳跃连接**，允许数据直接流向任何后续项
> - 引入**残差网络**，可以**使得网络层数非常深**，可以达到1000层以上。
>
> 有哪些经典网络模型使用了ResNet？
>
> YOLOV3、

#### 2、FCN

> ![](D:\Desktop\8期CV课\项目一(车道线分割)\images\车道线分割-week2课前PPT\车道线分割-week2课前PPT_21.jpg)
>
> **1、作用**
>
> - **恢复分辨率**；
> - 在FCN中，up-sampling采用双线性插值进行分辨率的提升，而这种提升是非学习的，采用解卷积来完成上采样的工作，就可以通过学习的方式得到更高的精度。
>
> 2、**缺点**
>
> 1. 卷积矩阵是稀疏的，因此大量的信息是无用的；
> 2. 求卷积矩阵的转置矩阵是非常耗费计算资源的。
>
> FCN使用了三种技术：
>
> - **卷积化（Convolutional）**
> - **上采样（Upsample）**------双线性插值
> - **跳跃结构（Skip Layer）**
>
> 1. 为什么需要FCN？
>
>    我们分类使用的网络通常会在最后连接几层全连接层，它会将原来二维的矩阵（图片）压扁成一维的，从而**丢失了空间信息**，最后训练输出一个标量，这就是我们的分类标签。
>
>    而图像语义分割的输出需要是个分割图，且不论尺寸大小，但是至少是二维的。所以，我们需要丢弃全连接层，换上全卷积层，而这就是全卷积网络了。

#### 3、U-Net

> ![](D:\Desktop\8期CV课\项目一(车道线分割)\images\U-Net-architecture.jpg)
>
> - **Encode-Decode**
>
> - **Overlap-title strategy**策略:
>
>   ​	图片太大我们每次取子图片，也解决了算力不足的问题，同时呢样本图片也增多了，一张就可以变多张图片了,子图片输入网络当中我们要考虑一个问题：周围像素可以通过镜像padding原图得到；还有就是原图边缘部分通过镜像padding得到---padding一些原图上来
>
>   这样的好处就是可以解决数据量不足的问题也解决了算力不足的问题
>
>   分类类别：2类--前景和背景，分布不均不平衡
>
>   U-Net只是在原图上进行padding，再没有其他padding

#### 4、DeepLab系列

> 膨胀卷积的目的：增加感受野的大小
>
> **DeepLab v1网络结构：**
>
> ![](D:\Desktop\8期CV课\项目一(车道线分割)\images\车道线分割-week4预习\车道线分割-week4预习_05.jpg)
>
> **DeepLab v2网络结构：**
>
> ![](D:\Desktop\8期CV课\项目一(车道线分割)\images\车道线分割-week4预习\车道线分割-week4预习_10.jpg)
>
> ![](D:\Desktop\8期CV课\项目一(车道线分割)\images\车道线分割-week4预习\车道线分割-week4预习_20.jpg)
>
> ------
>
> #### Dice loss
>
> Dice loss将语义分割的评价指标作为Loss，**Dice系数是一种集合相似度度量函数，通常用于计算两个样本的相似度，取值范围在[0,1]。**
>
> 计算公式如下：
>
> ![在这里插入图片描述](images\dice-loss.jpg)
>
> 就是**预测结果和真实结果的交乘上2**，除上**预测结果加上真实结果**。其值在0-1之间。越大表示预测结果和真实结果重合度越大。**所以Dice系数是越大越好。**
>
> 如果作为LOSS的话是越小越好，所以使得Dice loss = 1 - Dice，就可以将Loss作为语义分割的损失了。
>
> https://blog.csdn.net/weixin_44791964/article/details/111303419?spm=1001.2014.3001.5501
>
> ------
>
> 下面再阐述下deconv和dilated conv的区别：
>
> deconv的具体解释可参见[如何理解深度学习中的deconvolution networks？](https://www.zhihu.com/question/43609045/answer/132235276)，deconv的其中一个用途是做upsampling，即增大图像尺寸恢复分辨率。而dilated conv并不是做upsampling，而是增大感受野。
>
> 可以形象的做个解释：
>
> 对于标准的k*k卷积操作，stride为s，分三种情况：
>
> (1) s>1，即卷积的同时做了downsampling，卷积后图像尺寸减小；
>
> (2) s=1，普通的步长为1的卷积，比如在tensorflow中设置padding=SAME的话，卷积的图像输入和输出有相同的尺寸大小；
>
> (3) 0<s<1，fractionally( 微小地；极少地) strided convolution，相当于对图像做upsampling。比如s=0.5时，意味着在图像每个像素之间padding一个空白的像素后，stride改为1做卷积，得到的feature map尺寸增大一倍。
>
> 而dilated conv不是在像素之间padding空白的像素，而是在已有的像素上，skip掉一些像素，或者输入不变，**对conv的kernel参数中插一些0的weight**，达到一次卷积看到的空间范围变大的目的。
>
> ------
>
> **问题：**
>
> ​	作膨胀卷积是为了**增大感受野**(还有就是，卷积后feature map每个像素点对应原图的区域会大一些也就是代表原图的信息更多)，那为什么不直接使用较大的卷积核呢？那岂不是也能增大感受野？
>
> 答案：普通卷积的时候，将**stride设置为大于1**的数，也能增大感受野，但是这样的话，同时会产生downsampling，使得图像尺寸变小。
>
> ------
>
> 膨胀卷积的计算公式：	
>
> ​	卷积计算公式和普通工卷积计算公式一样，唯一的不同之处就是kernel_size尺寸要重新计算(因为膨胀了加了部分空白像素，卷积核在原来的额尺寸上增大了)
>
> ------
>
> **膨胀卷积是kernel裂开，反卷积是feature map裂开**

#### 5、Mask RCNN

> 参考链接：https://blog.csdn.net/weixin_44791964/article/details/104629135
>
> 源码：<https://github.com/bubbliiiing/mask-rcnn-keras>
>
> ![](images\Mask-RCNN.jpg)

YOLOV2:

![](images\TRUTH_BBOXES.jpg)

![TRUTH_BBOXES01](images\TRUTH_BBOXES01.jpg)

Remove FC layers: Can accept any size of inputs, enhance model robustness.

还有softmax公式以及求导等等都得掌握。

YOLOV3网络结构图：

![1630245956654](D:\AppData\Roaming\Typora\typora-user-images\1630245956654.png)



使用FPN的优点：

如上图所示，我们可以看到我们的**图像中存在不同尺寸的目标，而不同的目标具有不同的特征，利用浅层的特征就可以将简单的目标的区分开来；利用深层的特征可以将复杂的目标区分开来；**

另外可以获得高层语义信息，提高模型的鲁棒性。

- FPN 构架了一个可以进行端到端训练的特征金字塔；
- 通过CNN网络的层次结构高效的进行强特征计算；
- 通过结合bottom-up与top-down方法**获得较强的语义特征**，提高目标检测和实例分割在多个数据集上面的性能表现；
- FPN这种架构可以灵活地应用在不同地任务中去，包括目标检测、实例分割等；

对anchor的理解这篇博客帮助理解很大：

仔细研究一下：

https://blog.csdn.net/qq_43211132/article/details/102619661

这篇博客写的太好了，对于理解这个anchor帮助实在大的很。

选择anchor的形状：

yolov3使用k-means算法在训练集中**所有样本的真实框**（ground truth）中聚类，得到具有代表性形状的宽高（维度聚类）。但是具体几个anchor才是最合适的，作者采用实验的方式，分别用不同数量的anchor应用到模型，然后找出最优的在模型的复杂度和高召回率之间这种的那组anchor box，最终选出9个anchor box最佳。

而对于yolov3来说，输出为3个尺度的特征图，分别为13×13、26×26、52×52，对应着9个anchor，每个尺度均分3个anchor。
————————————————

**先平移(tx,ty)+再尺度缩放(tw,th)**

如何通过聚类算法kmeans得到anchor：

https://blog.csdn.net/qq_37541097/article/details/119647026

https://zhuanlan.zhihu.com/p/109968578

**训练时：**



![img](https://img-blog.csdnimg.cn/2018111520522436.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM0MTk5MzI2,size_16,color_FFFFFF,t_70)

用x,y坐标减去anchor box的x,y坐标得到偏移量好理解，为何要除以feature map上anchor box的宽和高呢？我认为可能是为了把绝对尺度变为相对尺度，毕竟作为偏移量，不能太大了对吧。而且不同尺度的anchor box如果都用Gx-Px来衡量显然不对，有的anchor box大有的却很小，都用Gx-Px会导致不同尺度的anchor box权重相同，**而大的anchor box肯定更能容忍大点的偏移量，小的anchor box对小偏移都很敏感，故除以宽和高可以权衡不同尺度下的预测坐标偏移量。**
————————————————
原文链接：https://blog.csdn.net/qq_34199326/article/details/84109828

**预测时：**

![img](https://img-blog.csdnimg.cn/20181115202416134.png)

这个公式tx,ty为何要sigmoid一下啊？前面讲到了在yolov3中没有让Gx - Cx后除以Pw得到tx，而是直接Gx - Cx得到tx，这样会有问题是导致tx比较大且很可能>1.(因为没有除以Pw归一化尺度)。用sigmoid将tx,ty压缩到[0,1]区间內，可以有效的确保目标中心处于执行预测的网格单元中，防止偏移过多。举个例子，我们刚刚都知道了网络不会预测边界框中心的确切坐标而是预测与预测目标的grid cell左上角相关的偏移tx,ty。如13*13的feature map中，某个目标的中心点预测为(0.4,0.7)，它的cx,cy即中心落入的grid cell坐标是(6,6)，则该物体的在feature map中的中心实际坐标显然是(6.4,6.7).这种情况没毛病，但若tx,ty大于1，比如(1.2,0.7)则该物体在feature map的的中心实际坐标是(7.2,6.7)，注意这时候该物体中心在这个物体所属grid cell外面了，但(6,6)这个grid cell却检测出我们这个单元格内含有目标的中心（yolo是采取物体中心归哪个grid cell整个物体就归哪个grid celll了），这样就矛盾了，因为左上角为(6,6)的grid cell负责预测这个物体，这个物体中心必须出现在这个grid cell中而不能出现在它旁边网格中，一旦tx,ty算出来大于1就会引起矛盾，因而必须归一化。

 看最后两行公式，tw为何要指数呀，这就好理解了嘛，因为tw,th是log尺度缩放到对数空间了，当然要指数回来，而且这样可以保证大于0。 至于左边乘以Pw或者Ph是因为tw=log(Gw/Pw)当然应该乘回来得到真正的宽高。
————————————————
原文链接：https://blog.csdn.net/qq_34199326/article/details/84109828



```python
# 对应关系          52x52                   26x26                    13x13
# mask =    0,      1,      2,      3,      4,      5,       6,       7,        8
# anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326
```

```python
# 阅读源码
主要函数：
YOLOLayer类：
compute_grid_offsets(self, grid_size, cuda=True)
build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres)
```

```python
# torch.stack()
# 假设是时间步T1的输出
T1 = torch.tensor([[1, 2, 3],
        		[4, 5, 6],
        		[7, 8, 9]])
# 假设是时间步T2的输出
T2 = torch.tensor([[10, 20, 30],
        		[40, 50, 60],
        		[70, 80, 90]])
————————————————
print(torch.stack((T1,T2),dim=0).shape)
print(torch.stack((T1,T2),dim=1).shape)
print(torch.stack((T1,T2),dim=2).shape)
print(torch.stack((T1,T2),dim=3).shape)
# outputs:
torch.Size([2, 3, 3])
torch.Size([3, 2, 3])
torch.Size([3, 3, 2])
'选择的dim>len(outputs)，所以报错'
IndexError: Dimension out of range (expected to be in range of [-3, 2], but got 3)

```

YOLOV2 PPT课件：

![](images\YOLOV2-子豪兄\Screenshot_20210831_121147_tv.danmaku.bili.jpg)

![Screenshot_20210831_122822_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_122822_tv.danmaku.bili.jpg)

![Screenshot_20210831_124722_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_124722_tv.danmaku.bili.jpg)

![Screenshot_20210831_125619_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_125619_tv.danmaku.bili.jpg)

![Screenshot_20210831_125910_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_125910_tv.danmaku.bili.jpg)

![Screenshot_20210831_130004_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_130004_tv.danmaku.bili.jpg)

![Screenshot_20210831_130102_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_130102_tv.danmaku.bili.jpg)

![Screenshot_20210831_130353_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_130353_tv.danmaku.bili.jpg)

![Screenshot_20210831_130403_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_130403_tv.danmaku.bili.jpg)

![Screenshot_20210831_130414_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_130414_tv.danmaku.bili.jpg)

![Screenshot_20210831_130438_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_130438_tv.danmaku.bili.jpg)

![Screenshot_20210831_130844_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_130844_tv.danmaku.bili.jpg)

![Screenshot_20210831_130917_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_130917_tv.danmaku.bili.jpg)

![Screenshot_20210831_131005_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_131005_tv.danmaku.bili.jpg)

![Screenshot_20210831_131050_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_131050_tv.danmaku.bili.jpg)

​										上图是YOLOV1对应的

![Screenshot_20210831_131132_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_131132_tv.danmaku.bili.jpg)

​										上图是YOLOV2对应的

![Screenshot_20210831_131332_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_131332_tv.danmaku.bili.jpg)

![Screenshot_20210831_131356_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_131356_tv.danmaku.bili.jpg)

![Screenshot_20210831_131433_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_131433_tv.danmaku.bili.jpg)

![Screenshot_20210831_131537_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_131537_tv.danmaku.bili.jpg)

![Screenshot_20210831_131559_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_131559_tv.danmaku.bili.jpg)

![Screenshot_20210831_131721_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_131721_tv.danmaku.bili.jpg)

![Screenshot_20210831_131912_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_131912_tv.danmaku.bili.jpg)

![Screenshot_20210831_132032_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_132032_tv.danmaku.bili.jpg)

![Screenshot_20210831_132133_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_132133_tv.danmaku.bili.jpg)

![Screenshot_20210831_132315_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_132315_tv.danmaku.bili.jpg)

![Screenshot_20210831_132406_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_132406_tv.danmaku.bili.jpg)

上图是为了说明使用sigmod函数来约束tx,ty值的原因，就是怕真实框和预测框位置相差太远而没法回归计算loss

![Screenshot_20210831_132501_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_132501_tv.danmaku.bili.jpg)

![Screenshot_20210831_132724_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_132724_tv.danmaku.bili.jpg)

![Screenshot_20210831_132807_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_132807_tv.danmaku.bili.jpg)

![Screenshot_20210831_132847_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_132847_tv.danmaku.bili.jpg)

![Screenshot_20210831_133534_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_133534_tv.danmaku.bili.jpg)

![Screenshot_20210831_133545_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_133545_tv.danmaku.bili.jpg)

![Screenshot_20210831_133635_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_133635_tv.danmaku.bili.jpg)

![Screenshot_20210831_133705_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_133705_tv.danmaku.bili.jpg)

![Screenshot_20210831_133748_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_133748_tv.danmaku.bili.jpg)

![Screenshot_20210831_134246_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_134246_tv.danmaku.bili.jpg)

![Screenshot_20210831_134259_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_134259_tv.danmaku.bili.jpg)

输入尺度越大，精度越高，但速度越慢；输入尺度越小，精度越低，但速度越快

![Screenshot_20210831_134607_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_134607_tv.danmaku.bili.jpg)

![Screenshot_20210831_134704_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_134704_tv.danmaku.bili.jpg)

![Screenshot_20210831_134727_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_134727_tv.danmaku.bili.jpg)

![Screenshot_20210831_134943_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_134943_tv.danmaku.bili.jpg)

![Screenshot_20210831_135033_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_135033_tv.danmaku.bili.jpg)

![Screenshot_20210831_135214_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_135214_tv.danmaku.bili.jpg)

![Screenshot_20210831_135244_tv.danmaku.bili](images\YOLOV2-子豪兄\Screenshot_20210831_135244_tv.danmaku.bili.jpg)



### NLP知识点总结

### 1、常见NLP算法总结

#### 1.1、RNN

简介：

RNN就是一个序列网络，当前时刻的输入由上一时刻的输出隐藏层和当前时刻的输入共同组成当前时刻的输入，如此一直循环就可以实现传递信息的作用，这样最后一层的隐藏层就包含了前面所有时刻的信息。

![](images\RNN01.png)

output1,hidden=f(x0,hidden) 

output2,hidden=f(x1,hidden) 

............如此循环就可以实现传递的作用

![](images\RNN03.jpg)

#### 1.2、LSTM

简介：

LSTM是RNN的一种变种，可以有效地解决RNN的梯度爆炸或者消失问题。LSTM的改进在于增加了新的记忆单元与门控机制。

- LSTM进入了一个新的记忆单元Ct，用于进行线性的循环信息传递，同时输出信息给隐藏层的外部状态ht。在每个时刻t, **ct记录了到当前时刻为止的历史信息**。

- LSTM引入门控机制来控制信息传递的路径，类似于数字电路中的门，0即关闭，1即开启。LSTM中的三个门为遗忘门ft，输入门it和输出门Ot。

  - ft控制上一个时刻的记忆单元ct-1需要遗忘多少信息。

  - it控制当前时刻的候选状态~ct有多少信息需要存储。
  - Ot控制当前时刻的记忆单元Ct有多少信息需要给输出给外部状态ht。

![image-20211025155036314](https://img-blog.csdnimg.cn/img_convert/aac51fedfa5b5dda1356bf4c7ff08b39.png)

![](images\LSTM01.jpg)

![](images\LSTM02.jpg)

![](images\LSTM03.jpg)

面试会问：你能不能将这个LSTM图画出来

做法：画图+公式

先写Ct的公式

再写ft的公式

再写it

再写Ct~

还有阀门都是sigmod函数

为什么还有tanh,我们这一时刻的输入可能有负向作用，所以我们

![](images\LSTM04.jpg)

LSTM参考链接：https://blog.csdn.net/qq_40922271/article/details/120961760

#### 1.3、GRU

![](images\GRU.jpg)

没有Wf了，GRU参数少了，减少过拟合了。

![](images\RNN-APPLICATION.jpg)

RNN，LSTM,GRU渐渐地已经淡出我们的历史舞台了，因为运算太慢了，数据量大的话特别慢

现在更多的是CNN和transformer等，可以并行计算。

#### 1.4、transformer

网络结构：transformer是个Encoder和Decoder的结构。

![](images\transformer02.jpg)



![](images\transformer03.jpg)

![](images\transformer01.jpg)

##### Encoder模块

- Encoder模块的结构和作用:
  - 经典的Transformer结构中的Encoder模块包含6个Encoder Block.
  - 每个Encoder Block包含一个多头自注意力层, 和一个前馈全连接层.
- 关于Encoder Block:
  - 在Transformer架构中, 6个一模一样的Encoder Block层层堆叠在一起, 共同组成完整的Encoder, 因此剖析一个Block就可以对整个Encoder的内部结构有清晰的认识.

##### 1、多头自注意力层(self-attention):

> - 首先来看self-attention的计算规则图:
>
> - ![](images\transformer05.jpg)
>
>   ![](images\transformer04.jpg)



![img](http://121.199.45.168:8022/img/picture_2.png)



------

> - 上述attention可以被描述为将query和key-value键值对的一组集合映射到输出, 输出被计算为values的加权和, 其中分配给每个value的权重由query与对应key的相似性函数计算得来. 这种attention的形式被称为**Scaled Dot-Product Attention**, 对应的数学公式形式如下:
> - 其实呢，这个Q=Embeddings x Wq得到，同理，K=Embeddings x Wk; V=Embeddings x Wv得到。



![img](http://121.199.45.168:8022/img/picture_3.png)



------

> - 所谓的**多头self-attention**层, 则是先将Q, K, V经过参数矩阵进行映射, 再做self-attention, 最后将结果拼接起来送入一个全连接层即可.



![img](http://121.199.45.168:8022/img/picture_1.png)



------

> - 上述的多头self-attention, 对应的数学公式形式如下:



![img](http://121.199.45.168:8022/img/picture_4.png)

![](images\transformer06-multi-head-attention.jpg)

------

> - 多头self-attention层的作用: 实验结果表明, **Multi-head可以在更细致的层面上提取不同head的特征**, 总体计算量和单一head相同的情况下, 提取特征的效果更佳.

------

##### 2、前馈全连接层模块

- 前馈全连接层模块, 由两个线性变换组成, 中间有一个Relu激活函数, 对应的数学公式形式如下:



![img](http://121.199.45.168:8022/img/picture_5.png)



------

> - 注意: 原版论文中的前馈全连接层, 输入和输出的维度均为d_model = 512, 层内的连接维度d_ff = 2048, 均采用4倍的大小关系.

------

> - 前馈全连接层的作用: **单纯的多头注意力机制并不足以提取到理想的特征, 因此增加全连接层来提升网络的能力.**

##### **Decoder模块**

- Decoder模块的结构和作用:
  - 经典的Transformer结构中的Decoder模块包含6个Decoder Block.
  - 每个Decoder Block包含三个子层.
    - 一个多头self-attention层
    - 一个Encoder-Decoder attention层
    - 一个前馈全连接层

------

##### 1、Decoder Block中的多头self-attention层

- Decoder中的多头self-attention层与Encoder模块一致, 但需要注意的是Decoder模块的多头self-attention需要做**look-ahead-mask**, 因为在预测的时候"不能看见未来的信息", 所以要将当前的token和之后的token全部mask.

------

##### 2、Decoder Block中的Encoder-Decoder attention层

- 这一层区别于自注意力机制的Q = K = V, 此处矩阵Q来源于Decoder端经过上一个Decoder Block的输出, 而矩阵K, V则来源于Encoder端的输出, 造成了Q != K = V的情况.
- 这样设计是为了让Decoder端的token能够给予Encoder端对应的token更多的关注.
- 通过计算Q和K的相似度(余弦向量的内积，内积越大相似度越大，相反则越小)来算V的权重，每一个V相加的时候就会根据权重值来进行相加。其中，Q,K,V长度都是一样的
- ![](images\QKV.jpg)

------

##### 3、Decoder Block中的前馈全连接层

- 此处的前馈全连接层和Encoder模块中的完全一样.

------

- Decoder Block中有2个注意力层的作用: **多头self-attention层是为了拟合Decoder端自身的信息, 而Encoder-Decoder attention层是为了整合Encoder和Decoder的信息.**
- 多头self-attention层也是为了模拟卷积层输出多通道的这样一个过程
- 自回归的概念：当前时刻的单词表示必须得有前面时刻的单词表示，一个一个来预测下一个单词。

------

**Add & Norm模块**

- Add & Norm模块接在每一个Encoder Block和Decoder Block中的每一个子层的后面. 具体来说Add表示残差连接, Norm表示LayerNorm.
  - 对于每一个Encoder Block, 里面的两个子层后面都有Add & Norm.
  - 对于每一个Decoder Block, 里面的三个子层后面都有Add & Norm.
  - 具体的数学表达形式为: LayerNorm(x + Sublayer(x)), 其中Sublayer(x)为子层的输出.

------

- Add残差连接的作用: 和其他神经网络模型中的残差连接作用一致, **都是为了将信息传递的更深, 增强模型的拟合能力**. 试验表明残差连接的确增强了模型的表现.

------

- Norm的作用: 随着网络层数的增加, 通过多层的计算后参数可能会出现过大, 过小, 方差变大等现象, 这会导致学习过程出现异常, 模型的收敛非常慢. 因此对每一层计算后的数值进行规范化可以提升模型的表现.

- Layer Norm 和 batch normal是有区别的：Layer Norm是把每一个batch的每一个行进行norm，batch normal是将每一个列(也就是每一个feature)做norm操作。蓝色表示batch normal，黄色表示layer norm。

  ![](images\transformer-norm.jpg)

------

**位置编码器Positional Encoding**

- Transformer中直接采用正弦函数和余弦函数来编码位置信息, 如下图所示:



![img](http://121.199.45.168:8022/img/picture_7.png)



------

- 需要注意: 三角函数应用在此处的一个重要的优点, 因为对于任意的PE(pos+k), 都可以表示为PE(pos)的线性函数, 大大方便计算. 而且周期性函数不受序列长度的限制, 也可以增强模型的泛化能力.

![img](http://121.199.45.168:8022/img/picture_6.png)

Decoder会做mask-Encoder-Decoder操作的原因是为了避免在t时刻看到t时刻以后的东西。具体操作就是将Qt时刻之后的值给它赋值成很大的一个负数。这样在后面做sofmax的时候指数结果就是0了。

![img](https://img-blog.csdnimg.cn/20190731112616766.PNG)

![](images\transformer06.jpg)

输入--->多头注意力机制抽取序列信息，信息汇聚--->MLP进行空间维度映射成我想要的维度和大小。

- 为什么要加Position-Encoding层，就是attention是没有时序信息的，像RNN系列都是有时序信息的。

#### 1.5、BERT

架构代码理解：https://blog.csdn.net/weixin_43178406/article/details/111916010

bert分类任务：embeddings+transformer+fc+softmax

##### 1.5.1、BERT的架构

- 总体架构: 如下图所示, 最左边的就是BERT的架构图, 可以很清楚的看到BERT采用了Transformer Encoder block进行连接, 因为是一个典型的双向编码模型.



![img](http://121.199.45.168:8022/img/BERT.png)



------

- 从上面的架构图中可以看到, 宏观上BERT分三个主要模块.
  - 最底层黄色标记的Embedding模块.
  - 中间层蓝色标记的Transformer模块.
  - 最上层绿色标记的预微调模块.

------

##### 1.5.2、Embedding模块:

BERT中的该模块是由三种Embedding共同组成而成, 如下图



![img](http://121.199.45.168:8022/img/BERT2.png)



------

> - Token Embeddings 是**词嵌入张量**, 第一个单词是CLS标志, 可以用于之后的分类任务.
> - Segment Embeddings 是**句子分段嵌入张量**, 是为了服务后续的两个句子为输入的预训练任务.
> - Position Embeddings 是**位置编码张量**, 此处注意和传统的Transformer不同, 不是三角函数计算的固定位置编码, 而是**通过学习得出来**的.
> - 整个Embedding模块的输出张量就是这3个张量的直接加和结果.

------

##### 1.5.3、双向Transformer模块

 BERT中只使用了经典Transformer架构中的Encoder部分, 完全舍弃了Decoder部分. 而两大预训练任务也集中体现在训练Transformer模块中.

------

##### 1.5.4、预微调模块

- 经过中间层Transformer的处理后, BERT的最后一层根据任务的不同需求而做不同的调整即可.
- 比如对于sequence-level的**`分类任务, BERT直接取第一个[CLS] token 的final hidden state, 再加一层全连接层后进行softmax来预测最终的标签.`**

------

> - 对于不同的任务, 微调都集中在预微调模块, 几种重要的NLP微调任务架构图展示如下



![img](http://121.199.45.168:8022/img/BERT3.png)



------

> - 从上图中可以发现, 在面对特定任务时, 只需要对预微调层进行微调, 就可以利用Transformer强大的注意力机制来模拟很多下游任务, 并得到SOTA的结果. (句子对关系判断, 单文本主题分类, 问答任务(QA), 单句贴标签(NER))

------

> - 若干可选的超参数建议如下:

```
Batch size: 16, 32
Learning rate (Adam): 5e-5, 3e-5, 2e-5
Epochs: 3, 4
```

------

#### 1.5.5、BERT的预训练任务

- BERT包含两个预训练任务:
  - 任务一: Masked LM (带mask的语言模型训练)
  - 任务二: Next Sentence Prediction (下一句话预测任务)

------

##### 1.5.5.1、Masked LM 

- 关于传统的语言模型训练, 都是采用left-to-right, 或者left-to-right + right-to-left结合的方式, 但这种单向方式或者拼接的方式提取特征的能力有限. 为此BERT提出一个**深度双向表达模型**(deep bidirectional representation). 即采用MASK任务来训练模型.
- 1: 在原始训练文本中, 随机的抽取15%的token作为参与MASK任务的对象.
- 2: 在这些被选中的token中, 数据生成器并不是把它们全部变成[MASK], 而是有下列3种情况.
  - 2.1: 在80%的概率下, 用[MASK]标记替换该token, 比如my dog is hairy -> my dog is [MASK]
  - 2.2: 在10%的概率下, 用一个随机的单词替换token, 比如my dog is hairy -> my dog is apple
  - 2.3: 在10%的概率下, 保持该token不变, 比如my dog is hairy -> my dog is hairy
- 3: 模型在训练的过程中, 并不知道它将要预测哪些单词? 哪些单词是原始的样子? 哪些单词被遮掩成了[MASK]? 哪些单词被替换成了其他单词? 正是在这样一种高度不确定的情况下, **反倒逼着模型快速学习该token的分布式上下文的语义**, 尽最大努力学习原始语言说话的样子. 同时因为原始文本中只有15%的token参与了MASK操作, 并不会破坏原语言的表达能力和语言规则.

------

##### 1.5.5.2、 Next Sentence Prediction 

- 在NLP中有一类重要的问题比如QA(Quention-Answer), NLI(Natural Language Inference), 需要模型能够很好的理解两个句子之间的关系, 从而需要在模型的训练中引入对应的任务. 在BERT中引入的就是Next Sentence Prediction任务. 采用的方式是输入句子对(A, B), 模型来预测句子B是不是句子A的真实的下一句话.
- 1: 所有参与任务训练的语句都被选中作为句子A.
  - 1.1: 其中50%的B是原始文本中真实跟随A的下一句话. (标记为IsNext, 代表正样本)
  - 1.2: 其中50%的B是原始文本中随机抽取的一句话. (标记为NotNext, 代表负样本)
- 2: 在任务二中, BERT模型可以在测试集上取得97%-98%的准确率.

#### 1.5.6、BPE原理

**「BPE」**（Byte Pair Encoding）

- BPE分词算法的由来

- BPE分词算法的流程

- - 词表构建
  - 语料编码
  - 语料解码

##### 1.5.6.1、**BPE分词算法的由来**

BPE算法，其目的是**「使用一些子词来编码数据」**。该方法已经成为了BERT等模型标准的数据预处理处理方式。

在机器翻译领域，模型训练之前一个很重要的步骤就是**「构建词表」**。对于英文语料，一个很自然的想法就是用训练语料中出现过的**「所有英语单词」**来构建词表，但是这样的方法存在两个问题：

- 训练语料中出现过的单词数目很多，这样的构造方式会使得词表变得很大，从而降低训练速度；
- 在模型测试中，很难处理罕见词或者训练过程中没有见过的词（OOV问题）。

另外一种方式是使用单个**「字符」**来构建词表。英文字符的个数是有限的，基于字符的方式可以有效缓解词表数目过大以及OOV的问题，但由于其粒度太细，丢失了很多单词本身所具有的语意信息。

为了解决上述问题，基于Subword（子词）的算法被提出，其中的代表就是BPE算法，**「BPE算法的分词粒度处于单词级别和字符级别之间」**。比如说单词"looked"和"looking"会被划分为"look"，"ed”，"ing"，这样在降低词表大小的同时也能学到词的语意信息。

##### 1.5.6.2、**BPE分词算法的流程**

BPE算法的核心主要分成三个部分：

- 词表构建
- 语料编码
- 语料解码

##### **词表构建**

词表构建是BPE算法的核心，其目的是**「根据训练语料」**来构建BPE算法的词表。算法的整体步骤如下所示：

1. 准备模型的训练语料
2. 确定**「期望的词表大小」**
3. 将训练语料中的所有单词拆分为字符序列，利用这些字符序列构建初始的词表
4. 统计训练语料中每一个连续字节对出现的频率，**「选择出现频率最高的字节对合并成新的subword，并更新词表」**
5. 重复第4步，直到词表大小达到我们设定的期望或者剩下的字节对出现频率最高为1

下面我们通过一个例子来搞懂BPE词表构建的过程。假设我们目前的训练语料中出现过的单词如下，我们构建初始词表：

![img](https://pic3.zhimg.com/80/v2-2e0816bc28d1a2c820368d448474ae76_720w.jpg)

值得注意的是，我们在每一个单词的后面都加入了一个新的字符`<\w>`来表示这个单词的结束。初始的词表大小为7，其为训练语料中所有出现过的字符。

我们之后发现`lo`这个字节对在训练语料中出现频率最高，为3次。我们更新词表，将`lo`作为新的子词加入词表，并删除在当前训练语料中不单独出现的字符`l`和`o`。

![img](https://pic2.zhimg.com/80/v2-bdf051f584c16169d79b0c821f7ea48d_720w.jpg)

之后我们发现`low`这个字节对在训练语料中出现频率最高，为3次。我们继续组合，将`low`加入词表中，并删去`lo`。需要注意的是，由于字符`w`在单词`newer`中仍然存在，因此不予删除。

![img](https://pic2.zhimg.com/80/v2-e4e95848240b8cb19cd9aa305c0acd3d_720w.jpg)

之后我们继续这个循环过程，在词表中加入`er`，并删去字符`r`

![img](https://pic2.zhimg.com/80/v2-285f065a6182c115f6f0552aabb57a49_720w.jpg)

我们一直循环这个过程，直到词表大小达到我们设定的期望或者剩下的字节对出现频率最高为1。

最终我们就得到了基于训练样本构建好的词表。

##### **语料编码**

词表构建好后，我们需要给训练语料中的单词进行编码。编码方式如下：

1. 我们首先**「将词表中所有的子词按照长度从大到小进行排序」**
2. 对于每一个给定的单词，我们遍历排序好的词表，寻找词表中的子词是否是该单词的子字符串。如果正好**「匹配」**，则输出当前子词，并对单词剩下的字符串继续匹配
3. 如果遍历完词表，单词中仍然有子字符串没有被匹配，那我们将其替换为一个特殊的子词，比如`<unk>`。

具个例子，假设我们现在构建好的词表为

```text
(“errrr</w>”, 
“tain</w>”, 
“moun”, 
“est</w>”, 
“high”, 
“the</w>”, 
“a</w>”)
```

对于给定的单词`mountain</w>`，其分词结果为：[`moun`, `tain</w>`]

##### **语料解码**

语料解码就是将所有的输出子词拼在一起，直到碰到结尾为`<\w>`。举个例子，假设模型输出为：

```text
["moun", "tain</w>", "high", "the</w>"]
```

那么其解码的结果为

```text
["mountain</w>", "highthe</w>"]
```

在本文中，我们一起学习了BPE的分词算法，该算法是**「利用子词来编码数据」**，已经成为目前机器翻译领域标准的预处理方式。

------

参考文章：https://zhuanlan.zhihu.com/p/383650769

#### 1.5.7、BLEU计算原理

BLEU全称是：**`a Method for Automatic Evaluation of Machine Translation`**(是一种用来评估机器翻译的评价指标)广泛出现在，文本生成的论文当中。
BLEU采用一种N-Gram的匹配规则，具体来说就是**比较译文和参考文献之间的N组词的相似度**：
举个例子：
Source Sentence: **今天天气不错**.
Target Sentence: **It is a nice day today.**
Candidate Sentence：**Today is a nice day**
**当你使用1-Gram匹配时：**
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210530183048700.png)可以发现，在你不考虑语序的前提下，Candidate中五个词命中了参考译文，于是Score(1-gram)=5/6

比如，以3-Gram举例子
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210530183257684.png)
可以看到Candidate中有两个三元词汇命中了Reference中的四个三元词汇，于是Score(3-gram)=2/4
N-gram匹配法则可以理解为，1-gram匹配时，代表Candidate中有多少个单词是正确翻译的，而2-gram~4-gram匹配就代表了Candidate的流畅程度

##### **BLEU的定义**

BLEU(Biligual Evaluation understudy):是一种**`基于单词精确度的相似性度量方式，可以用来比较Reference和Candidate中n元组共同出现的程度`**

##### **BLEU的好处：**

(1)速度快、成本低廉
(2)容易理解
(3)不受语种限制
(4)运用广泛

##### **BLEU的缺点**

但是BLEU也存在着十分严重的缺点：
(1)忽略同义词
(2)N-gram的机制会导致某项分数特别低
(3)BLEU不考虑意义
(4)BLEU不考虑句子的结构，很多时候，只要单词相同BLEU就会给出很高的分数
(5)BLEU不能够很好地处理形态丰富的语言
(6)BLEU与人类的判断并不相符合

```python
import math
from nltk.translate.bleu_score import sentence_bleu



reference = [['this', 'is', 'an','test']]
candidate = ['this', 'is', 'a', 'test']
score = sentence_bleu(reference, candidate)#计算真实的BLEU分数

#计算1-gram~4-gram的Pn分数
score_1 = sentence_bleu(reference, candidate, weights=(1,0,0,0))
score_2 = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))
score_3 = sentence_bleu(reference, candidate, weights=(0.33,0.33,0.33,0))
score_4 = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))

score_total = [score_1, score_2,score_3, score_4]
for score in score_total:
    print(score)

Pn_1 = math.log(score_1)
Pn_2 = math.log(score_2)
Pn_3 = math.log(score_3)
Pn_4 = math.log(score_4)

#因为Reference和Candidate的句子长度一致，所以BP惩罚因子为1
BP = 1
W_n = 1/4 #Wn是均匀加权，N的上限取值为4，即最多统计4-gram的精度
BLEU_score= BP * math.exp((Pn_1 + Pn_2 + Pn_3 + Pn_4)/W_n)

print(score)
print(BLEU_score)
12345678910111213141516171819202122232425262728293031
```

结合代码，我们可以观察一下BLEU分数的推导分析：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210601202918951.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1Njc3ODU0,size_16,color_FFFFFF,t_70)

![在这里插入图片描述](https://img-blog.csdnimg.cn/2021060120285476.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1Njc3ODU0,size_16,color_FFFFFF,t_70)

接下来再进行一个实例推导：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210601203022837.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQ1Njc3ODU0,size_16,color_FFFFFF,t_70)

参考文章：https://blog.csdn.net/qq_45677854/article/details/117401939

### 1.6、seq2seq



参考链接：https://blog.csdn.net/weixin_44388679/article/details/102575223

#### 1、分类网络

```python
# 导入若干工具包
import torch
import torch.nn as nn
import torch.nn.functional as F


# 定义一个简单的网络类
class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        # 定义第一层卷积神经网络, 输入通道维度=1, 输出通道维度=6, 卷积核大小3*3
        self.conv1 = nn.Conv2d(1, 6, 3)
        # 定义第二层卷积神经网络, 输入通道维度=6, 输出通道维度=16, 卷积核大小3*3
        self.conv2 = nn.Conv2d(6, 16, 3)
        # 定义三层全连接网络
        self.fc1 = nn.Linear(16 * 6 * 6, 120)
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # 在(2, 2)的池化窗口下执行最大池化操作
        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, self.num_flat_features(x))
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

    def num_flat_features(self, x):
        # 计算size, 除了第0个维度上的batch_size
        size = x.size()[1:]
        num_features = 1
        for s in size:
            num_features *= s
        return num_features


net = Net()
print(net)
```

